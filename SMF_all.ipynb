{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blackcaer/SMF-training/blob/main/SMF_all.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIr08PCme-MX",
        "outputId": "3d327a80-546c-4dbe-c806-b790bc38b646",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.13.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU nie jest dostępne.\n"
          ]
        }
      ],
      "source": [
        "#@title Initializing\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "\"\"\"gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')\"\"\"\n",
        "\n",
        "!mkdir -p /etc/OpenCL/vendors && echo \"libnvidia-opencl.so.1\" > /etc/OpenCL/vendors/nvidia.icd  # https://github.com/microsoft/LightGBM/issues/5914\n",
        "\n",
        "!pip install lightgbm --config-settings=cmake.define.USE_GPU=ON\n",
        "\n",
        "import lightgbm as lgb\n",
        "from IPython.display import display\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import random\n",
        "import psutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "from datetime import datetime\n",
        "from sys import getsizeof\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "from itertools import combinations\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_error,mean_absolute_percentage_error\n",
        "from sklearn.model_selection import train_test_split, GroupShuffleSplit, GridSearchCV\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "from scipy.fftpack import fft\n",
        "from scipy.stats import zscore\n",
        "\n",
        "from torch.cuda import get_device_name,is_available\n",
        "#from os import cpu_count\n",
        "\n",
        "#print(f\"Liczba rdzeni procesora: {cpu_count()} (realnych 2x mniej prawdopodobnie)\")\n",
        "gpu_available = is_available()\n",
        "\n",
        "if gpu_available:\n",
        "    print(\"GPU jest dostępne.\")\n",
        "    print(\"Nazwa GPU:\", get_device_name(0))\n",
        "else:\n",
        "    print(\"GPU nie jest dostępne.\")\n",
        "_features_added=False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "MYPEGa3cfjx1"
      },
      "outputs": [],
      "source": [
        "#@title Constants\n",
        "\n",
        "ITEMS_PHSM_JSON = '/content/drive/My Drive/SMF_files/items_phsm.json'\n",
        "PLAYER_COUNT_JSON = '/content/drive/My Drive/SMF_files/rust_player_count_interpolated.json'\n",
        "TRAIN_PATH = '/content/drive/My Drive/SMF_files/train_set.csv'\n",
        "VALID_PATH = '/content/drive/My Drive/SMF_files/validation_set.csv'\n",
        "\n",
        "TRAIN_PATH_TEST = '/content/drive/My Drive/SMF_files/train_set_test.csv'\n",
        "VALID_PATH_TEST = '/content/drive/My Drive/SMF_files/validation_set_test.csv'\n",
        "ITEMS_PHSM_JSON_TEST = '/content/drive/My Drive/SMF_files/items_phsm_test.json'\n",
        "\n",
        "SPIKES_TH=2\n",
        "SPIKES_TH_TEST=1.5\n",
        "SPIKES_PATH='/content/drive/My Drive/SMF_files/spikes_correction.csv'\n",
        "SPIKES_PATH_TEST='/content/drive/My Drive/SMF_files/spikes_correction_test.csv'\n",
        "\n",
        "MODEL_SAVES_PATH='/content/drive/MyDrive/SMF_files/model_saves'\n",
        "\n",
        "ITEMS_TO_EXCLUDE=['Metal Tree Door'] # 'Metal Tree Door' - like one of the rarest items in the game, very few sales since 2018"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ebAFze6PpAwH",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Params\n",
        "TEST_MODE = 1\n",
        "MAKE_SPIKES_CORR_FILE=0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title reducer\n",
        "\n",
        "\"\"\"reducing.py\n",
        "Author: Kirgsn, 2018\n",
        "\n",
        "Use like this:\n",
        ">>> import reducing\n",
        ">>> df = reducing.Reducer().reduce(df)\n",
        "\"\"\"\n",
        "from joblib import Parallel, delayed\n",
        "from fastprogress import master_bar, progress_bar\n",
        "\n",
        "#__all__ = ['Reducer']\n",
        "\n",
        "def measure_time_mem(func):\n",
        "    def wrapped_reduce(self, df, *args, **kwargs):\n",
        "        # pre\n",
        "        mem_usage_orig = df.memory_usage().sum() / self.memory_scale_factor\n",
        "        start_time = time.time()\n",
        "        # exec\n",
        "        ret = func(self, df, *args, **kwargs)\n",
        "        # post\n",
        "        mem_usage_new = ret.memory_usage().sum() / self.memory_scale_factor\n",
        "        end_time = time.time()\n",
        "        print(f'reduced df from {mem_usage_orig:.4f} MB '\n",
        "              f'to {mem_usage_new:.4f} MB '\n",
        "              f'in {(end_time - start_time):.2f} seconds')\n",
        "        gc.collect()\n",
        "        return ret\n",
        "    return wrapped_reduce\n",
        "\n",
        "\n",
        "class Reducer:\n",
        "    \"\"\"\n",
        "    Class that takes a dict of increasingly big numpy datatypes to transform\n",
        "    the data of a pandas dataframe into, in order to save memory usage.\n",
        "    \"\"\"\n",
        "    memory_scale_factor = 1024**2  # memory in MB\n",
        "\n",
        "    def __init__(self, conv_table=None, use_categoricals=True, n_jobs=-1):\n",
        "        \"\"\"\n",
        "        :param conv_table: dict with np.dtypes-strings as keys\n",
        "        :param use_categoricals: Whether the new pandas dtype \"Categoricals\"\n",
        "                shall be used\n",
        "        :param n_jobs: Parallelization rate\n",
        "        \"\"\"\n",
        "\n",
        "        self.conversion_table = \\\n",
        "            conv_table or {'int': [np.int8, np.int16, np.int32, np.int64],\n",
        "                           'uint': [np.uint8, np.uint16, np.uint32, np.uint64],\n",
        "                           'float': [np.float32, ]}\n",
        "        self.null_int = {   np.int8:  pd.Int8Dtype,\n",
        "                            np.int16: pd.Int16Dtype,\n",
        "                            np.int32: pd.Int32Dtype,\n",
        "                            np.int64: pd.Int64Dtype,\n",
        "                            np.uint8: pd.UInt8Dtype,\n",
        "                            np.uint16:pd.UInt16Dtype,\n",
        "                            np.uint32:pd.UInt32Dtype,\n",
        "                            np.uint64:pd.UInt64Dtype}\n",
        "\n",
        "        self.use_categoricals = use_categoricals\n",
        "        self.n_jobs = n_jobs\n",
        "\n",
        "    def _type_candidates(self, k):\n",
        "        for c in self.conversion_table[k]:\n",
        "            i = np.iinfo(c) if 'int' in k else np.finfo(c)\n",
        "            yield c, i\n",
        "\n",
        "    @measure_time_mem\n",
        "    def reduce(self, df, verbose=False):\n",
        "        \"\"\"Takes a dataframe and returns it with all data transformed to the\n",
        "        smallest necessary types.\n",
        "\n",
        "        :param df: pandas dataframe\n",
        "        :param verbose: If True, outputs more information\n",
        "        :return: pandas dataframe with reduced data types\n",
        "        \"\"\"\n",
        "        ret_list = Parallel(n_jobs=self.n_jobs, max_nbytes=None)(progress_bar(list(delayed(self._reduce)\n",
        "                                                (df[c], c, verbose) for c in\n",
        "                                                df.columns)))\n",
        "\n",
        "        del df\n",
        "        gc.collect()\n",
        "        return pd.concat(ret_list, axis=1)\n",
        "\n",
        "    def _reduce(self, s, colname, verbose):\n",
        "        try:\n",
        "            isnull = False\n",
        "            # skip NaNs\n",
        "            if s.isnull().any():\n",
        "                isnull = True\n",
        "            # detect kind of type\n",
        "            coltype = s.dtype\n",
        "            if np.issubdtype(coltype, np.integer):\n",
        "                conv_key = 'int' if s.min() < 0 else 'uint'\n",
        "            elif np.issubdtype(coltype, np.floating):\n",
        "                conv_key = 'float'\n",
        "                asint = s.fillna(0).astype(np.int64)\n",
        "                result = (s - asint)\n",
        "                result = np.abs(result.sum())\n",
        "                if result < 0.01:\n",
        "                    conv_key = 'int' if s.min() < 0 else 'uint'\n",
        "            else:\n",
        "                if isinstance(coltype, object) and self.use_categoricals:\n",
        "                    # check for all-strings series\n",
        "                    if s.apply(lambda x: isinstance(x, str)).all():\n",
        "                        if verbose: print(f'convert {colname} to categorical')\n",
        "                        return s.astype('category')\n",
        "                if verbose: print(f'{colname} is {coltype} - Skip..')\n",
        "                return s\n",
        "            # find right candidate\n",
        "            for cand, cand_info in self._type_candidates(conv_key):\n",
        "                if s.max() <= cand_info.max and s.min() >= cand_info.min:\n",
        "                    if verbose: print(f'convert {colname} to {cand}')\n",
        "                    if isnull:\n",
        "                        return s.astype(self.null_int[cand]())\n",
        "                    else:\n",
        "                        return s.astype(cand)\n",
        "\n",
        "            # reaching this code is bad. Probably there are inf, or other high numbs\n",
        "            print(f\"WARNING: {colname} doesn't fit the grid with \\nmax: {s.max()} \"\n",
        "                f\"and \\nmin: {s.min()}\")\n",
        "            print('Dropping it..')\n",
        "        except Exception as ex:\n",
        "            print(f'Exception for {colname}: {ex}')\n",
        "            return s\n",
        "\n",
        "def reduce_mem_usage(df):\n",
        "  return Reducer().reduce(df)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eNJBo6oqJqYv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "jSkQizQ7g2uZ"
      },
      "outputs": [],
      "source": [
        "#@title Helpers\n",
        "def show_metrics(y_pred_train, y_train, y_pred_test, y_test, y_pred_naive, label=\"[no label]\"):\n",
        "  \"\"\"Function to display metrics\"\"\"\n",
        "\n",
        "  print('=' * 8 + \" \" + label + \" \" + '=' * 8)\n",
        "\n",
        "  rmse_naive = mean_squared_error(y_test, y_pred_naive, squared=False)\n",
        "  #rmse_train = mean_squared_error(y_train, y_pred_train, squared=False)\n",
        "  rmse_test = mean_squared_error(y_test, y_pred_test, squared=False)\n",
        "\n",
        "  print(f'RMSE change: {(1 - rmse_test / rmse_naive) * -100:.2f}%   (naive->test)')\n",
        "\n",
        "\n",
        "  results = {\n",
        "      \"Metric\": [\"Accuracy\", \"RMSE\", \"MAE\", \"MAPE\", \"Max Error\", \"Median Absolute Error\"],\n",
        "      \"Train\": calc_accuracy(y_pred_train, y_train, \"Train accuracy:\"),\n",
        "      \"Valid\": calc_accuracy(y_pred_test, y_test, \"Validation accuracy:\"),\n",
        "      \"Naive\": calc_accuracy(y_pred_naive, y_test, \"Naive accuracy:\")\n",
        "  }\n",
        "\n",
        "  df = pd.DataFrame(results)\n",
        "  display(df)\n",
        "\n",
        "def calc_accuracy(pred, actual, label=\"\"):\n",
        "  \"\"\" Returns accuracy metrics: accuracy, rmse, mae, mape, max_error, median_absolute_error \"\"\"\n",
        "  N=3\n",
        "  errors = np.abs(pred - actual)\n",
        "  accuracy = round(100 * (1 - np.mean(errors / actual)), N)\n",
        "  max_error = round(np.max(errors), N)\n",
        "  std_deviation = round(np.std(errors), N)\n",
        "  mae = round(np.mean(errors), N)\n",
        "  median_absolute_error = round(np.median(errors), N)\n",
        "  mape = round(mean_absolute_percentage_error(actual, pred), N)\n",
        "  rmse = round(mean_squared_error(actual, pred, squared=False), N)\n",
        "\n",
        "  return accuracy, rmse, mae, mape, max_error, median_absolute_error\n",
        "\n",
        "\n",
        "def _print_columns_info(df, show_type=1, show_minmax=0, first_x_cols=50):\n",
        "    print(\"Column names:\", df.columns)\n",
        "    print(\"Columns number:\", len(df.columns))\n",
        "\n",
        "    lst = []\n",
        "    cols = ['Column']\n",
        "    if show_type:\n",
        "        cols.append('Type')\n",
        "    if show_minmax:\n",
        "        cols.append('Min')\n",
        "        cols.append('Max')\n",
        "\n",
        "    for column in df.columns:\n",
        "        row = {'Column': column}\n",
        "        if show_type:\n",
        "            row['Type'] = df[column].dtype\n",
        "        if show_minmax:\n",
        "            try:\n",
        "                row['Min'] = df[column].min()\n",
        "                row['Max'] = df[column].max()\n",
        "            except TypeError as e:\n",
        "                row['Min'] = None\n",
        "                row['Max'] = None\n",
        "\n",
        "        lst.append(row)\n",
        "\n",
        "    summary_df = pd.DataFrame(lst, columns=cols)\n",
        "\n",
        "    if first_x_cols:\n",
        "        summary_df = summary_df[:first_x_cols]\n",
        "    # Print the summary DataFrame\n",
        "    with pd.option_context('display.max_columns', None):\n",
        "        print(summary_df)\n",
        "\n",
        "def print_importances(model,start=0,print_tab=False,name=\"\",figsize=(10, 18),end=-1):\n",
        "  importance = model.feature_importance(importance_type='gain')  # 'split' lub 'gain'\n",
        "  feature_names = model.feature_name()\n",
        "\n",
        "  trim=0\n",
        "  importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
        "  importance_df = importance_df.sort_values(by='Importance', ascending=False)[start:end]\n",
        "\n",
        "  # Wizualizacja\n",
        "  print(f\"Trimmed first {trim} features to see better\")\n",
        "  plt.figure(figsize=figsize)\n",
        "  plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
        "  plt.xlabel('Importance')\n",
        "  plt.title(f'Feature Importance {name}')\n",
        "  plt.style.use('dark_background')\n",
        "  plt.gca().invert_yaxis()\n",
        "  plt.show()\n",
        "  pd.set_option('display.max_rows', None)\n",
        "  if print_tab:\n",
        "    display(importance_df)\n",
        "\n",
        "def aggregate_pricehistories(pricehistories):\n",
        "    interpolated_histories = []\n",
        "    max_date = max(df.index.max() for df in pricehistories)\n",
        "    for df in pricehistories:\n",
        "        df_resampled = df.resample('D').interpolate(method='linear')\n",
        "        full_date_range = pd.date_range(start=df_resampled.index.min(), end=max_date, freq='D')\n",
        "        df_reindexed = df_resampled.reindex(full_date_range)\n",
        "        df_filled = df_reindexed.ffill()\n",
        "\n",
        "        interpolated_histories.append(df_filled)\n",
        "\n",
        "    all_data = pd.concat(interpolated_histories)\n",
        "\n",
        "    sum = all_data.groupby(all_data.index).sum()\n",
        "    med = all_data.groupby(all_data.index).median()\n",
        "    mean = all_data.groupby(all_data.index).mean()\n",
        "\n",
        "    #plot_pricehistory(interpolated_histories,end_names,\"Test\",reset_index=0)\n",
        "    plot_pricehistory([med[:],mean[:]],['med','mean'],\"Test\")\n",
        "    plot_pricehistory([sum[:]],['sum'],\"Test\",1)\n",
        "    return sum,med,mean\n",
        "\n",
        "def col_from_idx(df,idx_name,pos=None):\n",
        "  df[idx_name] = df.index.get_level_values(idx_name)\n",
        "  if pos is not None:\n",
        "    cols = df.columns.tolist()\n",
        "    cols.insert(pos, cols.pop(cols.index(idx_name)))\n",
        "    df = df[cols]\n",
        "\n",
        "def rmse_top_items_analysis(y_actual,y_pred,data_mapped,show_top_items=False):\n",
        "  squared_errors = np.power(np.abs(y_actual - y_pred),2)\n",
        "\n",
        "  data_mapped_with_errors = data_mapped.assign(squared_error=squared_errors.values)\n",
        "  avg_rmse_per_item = np.sqrt(data_mapped_with_errors.groupby('name')['squared_error'].mean())\n",
        "\n",
        "  top_item_rmse = avg_rmse_per_item.nlargest(10)\n",
        "  if show_top_items:\n",
        "    print(\"Top items with largest rmse:\")\n",
        "    print(top_item_rmse)\n",
        "\n",
        "  items_to_exclude = top_item_rmse.index\n",
        "  mask = ~data_mapped.index.get_level_values('name').isin(items_to_exclude)\n",
        "\n",
        "  filtered_data_mapped = data_mapped[mask]\n",
        "  filtered_y_actual = y_actual[mask]\n",
        "  filtered_y_pred = y_pred[mask]\n",
        "  rmse_without_top_items = mean_squared_error(filtered_y_actual, filtered_y_pred,squared=False)\n",
        "  rmse_normal = mean_squared_error(y_actual, y_pred, squared=False)\n",
        "  print(f'\\nRMSE without top items: {rmse_without_top_items:.4f} (Change {(rmse_without_top_items/rmse_normal-1)*100:.2f}%)\\n')\n",
        "  return top_item_rmse\n",
        "\n",
        "def plot_pricehistory(pricehistories:list,labels:list,title,day_interval=60,relative_x_axis=False,figsize=(16, 6)):\n",
        "  plt.style.use('dark_background')\n",
        "  plt.figure(figsize=figsize)\n",
        "  ax = plt.gca()\n",
        "  for i in range(len(pricehistories)):\n",
        "    if relative_x_axis:\n",
        "      days_from_start = (pricehistories[i].index - pricehistories[i].index[0]).days\n",
        "\n",
        "      plt.plot(days_from_start, pricehistories[i].values, label=labels[i])\n",
        "      ax.set_xticks(days_from_start[::day_interval])\n",
        "      ax.set_xticklabels(days_from_start[::day_interval])\n",
        "    else:\n",
        "      plt.plot(pricehistories[i], label=labels[i])\n",
        "      #ax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
        "      ax.xaxis.set_major_locator(mdates.DayLocator(interval=day_interval))\n",
        "      ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "\n",
        "  ax.grid(True)\n",
        "  ax.grid(color='gray', linestyle='--', linewidth=0.5)\n",
        "  plt.xticks(rotation=45)\n",
        "  plt.xlabel('Date' if not relative_x_axis else 'Days from Start')\n",
        "  plt.ylabel('Price')\n",
        "  plt.title(title)\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "def plot_predictions_vs_actuals(test_dataset, predictions, item_name,label_main=\"\",label1='real',label2='pred'):\n",
        "  item_data = test_dataset.xs(item_name, level='name',drop_level=False)\n",
        "\n",
        "  if len(item_data)==0:\n",
        "    print(f\"{item_name} is not in given data\")\n",
        "    return\n",
        "\n",
        "  item_actuals = item_data['price']\n",
        "  item_predictions = predictions[item_data.index]\n",
        "\n",
        "  # Połącz dane w DataFrame\n",
        "  plot_df = pd.DataFrame({'date': item_data.index.get_level_values('date'), 'actual': item_actuals, 'predicted': item_predictions})\n",
        "\n",
        "  rmse = mean_squared_error(item_actuals, item_predictions, squared=False)\n",
        "\n",
        "  print(f'RMSE {item_name}: {rmse}')\n",
        "  # Utwórz wykres\n",
        "  plt.style.use('dark_background')\n",
        "  plt.figure(figsize=(12, 6))\n",
        "  plt.plot(plot_df['date'], plot_df['actual'], label=label1)\n",
        "  plt.plot(plot_df['date'], plot_df['predicted'], label=label2)\n",
        "\n",
        "  # Dodaj etykiety i tytuł\n",
        "  plt.xticks(rotation=45)\n",
        "  plt.xlabel('Data')\n",
        "  plt.ylabel('Cena')\n",
        "  plt.title(f'{item_name}: {label_main} {label1} vs {label2}')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Mey58tCLfj0H",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title DataPrepper\n",
        "class DataPrepper:\n",
        "    def __init__(self):\n",
        "        self.val_df = None\n",
        "        self.train_df = None\n",
        "        self.items_phsm_df = None\n",
        "        self.rust_player_count_df = None\n",
        "        self.dataset: pd.DataFrame = None\n",
        "        self.columns_to_drop = [\n",
        "            'previewUrl', 'views', 'timeCreated', 'timeRefreshed', 'isAvailableOnStore', 'creatorName',\n",
        "            'appId',\n",
        "            'id', 'nameId','hasGlow', 'hasCutout','timeAccepted'] # 'hasGlow', 'hasCutout' - no gain, no splits\n",
        "        self.desired_column_order = ['date', 'price', 'volume', 'name', 'playerCount', 'supplyTotalEstimated',\n",
        "                                     'storePrice',\n",
        "                                     'glowRatio',  'cutoutRatio', 'hasGlowSights', 'facepunchSkin',\n",
        "                                     'itemType',\n",
        "                                     'itemCollection',\n",
        "                                     ]\n",
        "\n",
        "    def _main(self):\n",
        "        raise NotImplementedError(\"Func only for tests\")\n",
        "        self.load_data()\n",
        "\n",
        "        self.preprocess_data()\n",
        "\n",
        "        self.dataset = self.add_features(self.dataset)\n",
        "\n",
        "        _print_columns_info(self.dataset)\n",
        "\n",
        "        # self.train_df, self.val_df = train_test_split(self.dataset, test_size=0.2, shuffle=False, stratify=None)\n",
        "\n",
        "        # self.save_dataset()\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"\n",
        "        Loads self.items_phsm_df and self.rust_player_count_df from files.\n",
        "        \"\"\"\n",
        "        with open(ITEMS_PHSM_JSON if not TEST_MODE else ITEMS_PHSM_JSON_TEST, 'r') as f:\n",
        "            items_phsm = json.load(f)\n",
        "\n",
        "        with open(PLAYER_COUNT_JSON, 'r') as f:\n",
        "            rust_player_count_json = json.load(f)\n",
        "\n",
        "        items_phsm = [item for item in items_phsm if item['name'] not in ITEMS_TO_EXCLUDE]\n",
        "\n",
        "        phsm_records = self.unfold_phsm(items_phsm)\n",
        "\n",
        "        self.items_phsm_df = pd.DataFrame(phsm_records)\n",
        "\n",
        "        self.rust_player_count_df = pd.DataFrame(rust_player_count_json)\n",
        "\n",
        "        print(\"Loaded\")\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        \"\"\"\n",
        "        Prepares, merges data and assigns it to self.dataset\n",
        "        \"\"\"\n",
        "\n",
        "        self.prep_player_count()\n",
        "        self.prep_items_phsm()\n",
        "        #display(self.dataset.head())\n",
        "        #display(self.dataset.head())\n",
        "\n",
        "        merged_df = pd.merge(self.items_phsm_df, self.rust_player_count_df, on='date',\n",
        "                             how='left')\n",
        "\n",
        "        ordered_df = merged_df.reindex(columns=self.desired_column_order)\n",
        "\n",
        "        #ordered_df.drop(columns=['timeAccepted'], inplace=True,errors='ignore')  # Drop unused columns\n",
        "\n",
        "        self.dataset = ordered_df\n",
        "\n",
        "    @staticmethod\n",
        "    def unfold_phsm(items_phsm):\n",
        "        phsm_records = []\n",
        "        for item in items_phsm:\n",
        "            for phsm_entry in item['phsm']:\n",
        "                record = {k: v for k, v in item.items() if k != 'phsm'}\n",
        "                record.update(phsm_entry)\n",
        "                phsm_records.append(record)\n",
        "        return phsm_records\n",
        "\n",
        "    def prep_player_count(self):\n",
        "        self.rust_player_count_df = self.rust_player_count_df.rename(columns={'Date': 'date'})\n",
        "        self.rust_player_count_df = self.rust_player_count_df.rename(columns={'Player_count': 'playerCount'})\n",
        "        self.rust_player_count_df['date'] = pd.to_datetime(self.rust_player_count_df['date'])\n",
        "\n",
        "        return self.rust_player_count_df\n",
        "\n",
        "    def prep_items_phsm(self):\n",
        "        self.items_phsm_df.rename(columns={'median': 'price'},inplace=True)\n",
        "        self.items_phsm_df['date'] = pd.to_datetime(self.items_phsm_df['date'])\n",
        "        self.items_phsm_df['timeAccepted'] = pd.to_datetime(self.items_phsm_df['timeAccepted'], errors='coerce')\n",
        "        self.items_phsm_df['date'] = pd.to_datetime(self.items_phsm_df['date'])\n",
        "        self.items_phsm_df.drop(columns=self.columns_to_drop,inplace=True)  # Delete unnecessary columns\n",
        "        self.items_phsm_df = self.items_phsm_df.sort_values(by=['name', 'date'])\n",
        "\n",
        "\n",
        "        self.items_phsm_df.reset_index(drop=True)\n",
        "\n",
        "        def normalize_spikes(group, threshold):\n",
        "            name = group.iloc[0]['name']\n",
        "            prices = group['price'].to_numpy()\n",
        "            corrections = []\n",
        "            num_spikes = 0\n",
        "            i = 3\n",
        "\n",
        "            while i < len(group) - 3:\n",
        "                current_price = prices[i]\n",
        "                window = prices[i-4:i+4] # 4 before i and i + 3 after i, so indexes are correct\n",
        "                median = np.median(window)\n",
        "\n",
        "                if current_price > threshold * median:\n",
        "                    num_spikes += 1\n",
        "                    idx=group.index[i]\n",
        "                    corrections.append((idx, np.mean(window)))  # index and new price\n",
        "                i += 1\n",
        "\n",
        "            if num_spikes:\n",
        "                print(f\"Prepared normalization for {num_spikes} spikes for {name}\")\n",
        "\n",
        "            return corrections\n",
        "\n",
        "        if MAKE_SPIKES_CORR_FILE:\n",
        "            corrections = self.items_phsm_df.groupby('name').apply(normalize_spikes, threshold=SPIKES_TH if not TEST_MODE else SPIKES_TH_TEST).sum()#.sum().tolist()\n",
        "\n",
        "            corrections_df = pd.DataFrame(corrections, columns=['index', 'new_price'])\n",
        "            corrections_df.to_csv(SPIKES_PATH if not TEST_MODE else SPIKES_PATH_TEST, index=False)\n",
        "        #return self.items_phsm_df.groupby('name').apply(normalize_spikes,threshold=3)\n",
        "\n",
        "        #return self.items_phsm_df\n",
        "\n",
        "    @staticmethod\n",
        "    def add_features(dataset,change_original=False):\n",
        "        raise NotImplementedError(\"Run window with add_features implementation first\") #Add features is often changed so it is implemented in other window so I don't have to create new object every time i change it\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "GDtjDISSgSqA"
      },
      "outputs": [],
      "source": [
        "#@title Helpers 2\n",
        "def split_dataset(X,y, test_size):\n",
        "\n",
        "    splitter = GroupShuffleSplit(test_size=test_size, n_splits=1, random_state=421)\n",
        "\n",
        "    groups = X.index.get_level_values('name')\n",
        "\n",
        "    train_idx, test_idx = next(splitter.split(X, y, groups))\n",
        "\n",
        "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def autosave_model(model):\n",
        "  folder_path=MODEL_SAVES_PATH\n",
        "  curr_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "  file_path = os.path.join(folder_path, f\"lightgbm_model_{curr_time}.txt\")\n",
        "  model.save_model(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "collapsed": true,
        "id": "nQFIxONOfj55",
        "outputId": "b06454a5-017b-43be-fd76-5022255db9d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='12' class='' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [12/12 00:07&lt;00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reduced df from 0.4959 MB to 0.2274 MB in 8.02 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='2' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [2/2 00:00&lt;00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reduced df from 0.0593 MB to 0.0445 MB in 0.45 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        date  price  volume         name  playerCount  supplyTotalEstimated  \\\n",
              "0 2024-05-30   6.06       6  Abyss Boots       123565                 10236   \n",
              "1 2024-05-31   2.16      25  Abyss Boots       130051                 10236   \n",
              "2 2024-06-01   2.07      17  Abyss Boots       125705                 10236   \n",
              "3 2024-06-02   2.08      13  Abyss Boots       122606                 10236   \n",
              "4 2024-06-03   1.96      22  Abyss Boots       115998                 10236   \n",
              "\n",
              "   storePrice  glowRatio  cutoutRatio  hasGlowSights  facepunchSkin itemType  \\\n",
              "0        1.49        0.0          0.0          False          False    Boots   \n",
              "1        1.49        0.0          0.0          False          False    Boots   \n",
              "2        1.49        0.0          0.0          False          False    Boots   \n",
              "3        1.49        0.0          0.0          False          False    Boots   \n",
              "4        1.49        0.0          0.0          False          False    Boots   \n",
              "\n",
              "  itemCollection  \n",
              "0          Abyss  \n",
              "1          Abyss  \n",
              "2          Abyss  \n",
              "3          Abyss  \n",
              "4          Abyss  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c36cc3c1-e578-4cd0-a4e0-056b9c33114a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>price</th>\n",
              "      <th>volume</th>\n",
              "      <th>name</th>\n",
              "      <th>playerCount</th>\n",
              "      <th>supplyTotalEstimated</th>\n",
              "      <th>storePrice</th>\n",
              "      <th>glowRatio</th>\n",
              "      <th>cutoutRatio</th>\n",
              "      <th>hasGlowSights</th>\n",
              "      <th>facepunchSkin</th>\n",
              "      <th>itemType</th>\n",
              "      <th>itemCollection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2024-05-30</td>\n",
              "      <td>6.06</td>\n",
              "      <td>6</td>\n",
              "      <td>Abyss Boots</td>\n",
              "      <td>123565</td>\n",
              "      <td>10236</td>\n",
              "      <td>1.49</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>Boots</td>\n",
              "      <td>Abyss</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2024-05-31</td>\n",
              "      <td>2.16</td>\n",
              "      <td>25</td>\n",
              "      <td>Abyss Boots</td>\n",
              "      <td>130051</td>\n",
              "      <td>10236</td>\n",
              "      <td>1.49</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>Boots</td>\n",
              "      <td>Abyss</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2024-06-01</td>\n",
              "      <td>2.07</td>\n",
              "      <td>17</td>\n",
              "      <td>Abyss Boots</td>\n",
              "      <td>125705</td>\n",
              "      <td>10236</td>\n",
              "      <td>1.49</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>Boots</td>\n",
              "      <td>Abyss</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2024-06-02</td>\n",
              "      <td>2.08</td>\n",
              "      <td>13</td>\n",
              "      <td>Abyss Boots</td>\n",
              "      <td>122606</td>\n",
              "      <td>10236</td>\n",
              "      <td>1.49</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>Boots</td>\n",
              "      <td>Abyss</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2024-06-03</td>\n",
              "      <td>1.96</td>\n",
              "      <td>22</td>\n",
              "      <td>Abyss Boots</td>\n",
              "      <td>115998</td>\n",
              "      <td>10236</td>\n",
              "      <td>1.49</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>Boots</td>\n",
              "      <td>Abyss</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c36cc3c1-e578-4cd0-a4e0-056b9c33114a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c36cc3c1-e578-4cd0-a4e0-056b9c33114a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c36cc3c1-e578-4cd0-a4e0-056b9c33114a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-848f87f3-933e-402a-9f3d-6b82605638b8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-848f87f3-933e-402a-9f3d-6b82605638b8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-848f87f3-933e-402a-9f3d-6b82605638b8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(dp\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2024-05-30 00:00:00\",\n        \"max\": \"2024-06-03 00:00:00\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2024-05-31 00:00:00\",\n          \"2024-06-03 00:00:00\",\n          \"2024-06-01 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.7869191363908998,\n        \"min\": 1.96,\n        \"max\": 6.06,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2.16,\n          1.96,\n          2.07\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"volume\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7,\n        \"min\": 6,\n        \"max\": 25,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          25,\n          22,\n          17\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Abyss Boots\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"playerCount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5119,\n        \"min\": 115998,\n        \"max\": 130051,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          130051\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"supplyTotalEstimated\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 10236,\n        \"max\": 10236,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          10236\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"storePrice\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.49,\n        \"max\": 1.49,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.49\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"glowRatio\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cutoutRatio\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hasGlowSights\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"facepunchSkin\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"itemType\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Boots\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"itemCollection\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Abyss\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title Load\n",
        "\n",
        "\n",
        "dp = DataPrepper()\n",
        "dp.load_data()\n",
        "dp.preprocess_data()\n",
        "\n",
        "# reduce_mem_usage in other places fucks something up, doesn't give much and is slow\n",
        "dp.items_phsm_df= reduce_mem_usage(dp.items_phsm_df)\n",
        "dp.rust_player_count_df = reduce_mem_usage(dp.rust_player_count_df)\n",
        "\n",
        "display(dp.dataset.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "Z03xbl2x3giZ"
      },
      "outputs": [],
      "source": [
        "#@title Update prices from file (spikes)\n",
        "\n",
        "def update_prices_from_file(df, correction_file):\n",
        "    # Wczytanie poprawek\n",
        "    corrections = pd.read_csv(correction_file)\n",
        "\n",
        "    # Ensure the correction DataFrame has appropriate types and indices\n",
        "    corrections.set_index('index', inplace=True)\n",
        "\n",
        "    # Update prices in the main DataFrame using the corrections\n",
        "    df=df.copy()\n",
        "    df.update(corrections[['new_price']].rename(columns={'new_price': 'price'}))\n",
        "\n",
        "    return df\n",
        "\n",
        "names=['Nocturnal Beast AR','Wooden Base Box','Bone Armor Bandana']  #Snakebite SAR\n",
        "\n",
        "corrections = pd.read_csv(SPIKES_PATH if not TEST_MODE else SPIKES_PATH_TEST)\n",
        "itemsphsm_before=dp.items_phsm_df.copy()     # !!!!!!!!!!!!\n",
        "\n",
        "itemsphsm_after = update_prices_from_file(itemsphsm_before, SPIKES_PATH if not TEST_MODE else SPIKES_PATH_TEST)\n",
        "\n",
        "itemsphsm_after.set_index(['name','date'],inplace=True)\n",
        "itemsphsm_after.sort_index(inplace=True)\n",
        "\n",
        "names=set()\n",
        "for i in corrections[['index']].values:\n",
        "  names.add(itemsphsm_before.loc[i].reset_index()['name'][0])\n",
        "names=list(names)\n",
        "\n",
        "itemsphsm_before.set_index(['name','date'],inplace=True)\n",
        "itemsphsm_before.sort_index(inplace=True)\n",
        "\n",
        "for name in names[::20]:\n",
        "  b=itemsphsm_before.loc[name][['price']]\n",
        "  a=itemsphsm_after.loc[name][['price']]\n",
        "  #plot_pricehistory([b,a],['before','after'],f'spikes {name}',figsize=(12,6))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iPtiepc6bUxa",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Def add_features\n",
        "\n",
        "\n",
        "def add_features(dataset):\n",
        "  to_interpolate = ['price', 'volume', 'playerCount']\n",
        "  bool_columns = []#[ 'facepunchSkin', 'is_weekend']#'hasGlowSights','hasGlow', 'hasCutout',\n",
        "  int_columns = ['year', 'month', 'day', 'volume', 'playerCount', 'supplyTotalEstimated', 'weekday']\n",
        "  categorical_columns = ['itemType', 'itemCollection']\n",
        "\n",
        "  columns_to_keep = ['price', 'volume', 'playerCount', 'supplyTotalEstimated', 'storePrice', 'itemType', 'itemCollection'] # , 'glowRatio', 'cutoutRatio', 'facepunchSkin'\n",
        "\n",
        "  dataset = dataset[columns_to_keep]\n",
        "\n",
        "  original_column_order = dataset.columns.tolist()\n",
        "\n",
        "  # Interpolating data\n",
        "  interpolated_data = dataset.groupby('name')[to_interpolate].apply(\n",
        "      lambda group: group.reset_index('name', drop=True).asfreq('D').interpolate(method='linear'))\n",
        "\n",
        "  not_interpolated_data = dataset.groupby('name')[dataset.columns.difference(to_interpolate)].apply(\n",
        "      lambda group: group.reset_index('name', drop=True).asfreq('D'))\n",
        "\n",
        "  dataset = pd.concat([interpolated_data, not_interpolated_data], axis=1)[original_column_order]\n",
        "\n",
        "  def compute_features(group):\n",
        "    lags_price = [2,3, 4, 5, 6, 7, 14, 21]#[2, 3, 4, 5, 6, 7, 14, 21]\n",
        "    lags_vol = [1, 2, 3, 7]#[1, 2, 3, 7]\n",
        "    lags_player_count=[0,7,30]\n",
        "    windows_price = [3, 7, 14, 21]\n",
        "    windows_volume = [3, 21]\n",
        "    windows_player_count=[7,14]\n",
        "    ewma_windows = [7, 14, 21, 30]\n",
        "\n",
        "    features = {}\n",
        "\n",
        "    for lag in lags_price:\n",
        "      features[f'price_lag{lag}'] = group['price'].shift(lag)\n",
        "\n",
        "    for lag in lags_vol:\n",
        "      features[f'vol_lag{lag}'] = group['volume'].shift(lag)\n",
        "\n",
        "    for lag in lags_player_count:\n",
        "      lag=lag+1   # lag+1 because rolling takes current data too, that'd be a data leak\n",
        "      for window in windows_player_count:\n",
        "\n",
        "        if len(group)+lag >= window:  # Ensure enough data points for rolling operations\n",
        "          roll = group[['playerCount']]['playerCount'].shift(lag).rolling(window=window)\n",
        "          features[f'playerCount_roll_mean_{window}_lag_{lag-1}'] = roll.mean()\n",
        "        else:\n",
        "          features[f'playerCount_roll_mean_{window}_lag_{lag-1}'] = np.nan\n",
        "\n",
        "    for window in windows_price:\n",
        "      if len(group) >= window:  # Ensure enough data points for rolling operations\n",
        "          roll = group[['price']].shift(1).rolling(window=window)\n",
        "          features.update({\n",
        "            f'price_roll_mean_{window}': roll['price'].mean(),\n",
        "            f'price_roll_std_{window}': roll['price'].std(),\n",
        "            f'price_roll_var_{window}': roll['price'].var(),\n",
        "            f'price_roll_sum_{window}': roll['price'].sum(),\n",
        "          })\n",
        "      else:\n",
        "        features.update({\n",
        "            f'price_roll_mean_{window}': np.nan,\n",
        "            f'price_roll_std_{window}':np.nan,\n",
        "            f'price_roll_var_{window}':np.nan,\n",
        "            f'price_roll_sum_{window}': np.nan,\n",
        "        })\n",
        "\n",
        "    for window in windows_volume:\n",
        "      if len(group) >= window:  # Ensure enough data points for rolling operations\n",
        "          roll = group[['volume']].shift(1).rolling(window=window)\n",
        "          features.update({\n",
        "            f'vol_roll_mean_{window}': roll['volume'].mean(),\n",
        "          })\n",
        "      else:\n",
        "        features.update({\n",
        "            f'vol_roll_mean_{window}': np.nan,\n",
        "        })\n",
        "\n",
        "    features['price_trend'] = np.arange(len(group)) * group['price'].shift(1).pct_change().fillna(0)\n",
        "    features['volume_trend'] = np.arange(len(group)) * group['volume'].shift(1).pct_change().fillna(0)\n",
        "\n",
        "    for window in ewma_windows:\n",
        "      features[f'price_ewma_{window}'] = group['price'].shift(1).ewm(span=window, adjust=False).mean()\n",
        "      #features[f'vol_ewma_{window}'] = group['volume'].shift(1).ewm(span=window, adjust=False).mean()\n",
        "\n",
        "    features['price_cumsum'] = group['price'].shift(1).cumsum()\n",
        "    features['volume_cumsum'] = group['volume'].shift(1).cumsum()\n",
        "\n",
        "    collection_group = group.groupby('itemCollection',observed=True)\n",
        "\n",
        "    for window in windows_price:\n",
        "      if len(group) >= window:\n",
        "        # Rolling mean for price and volume across the collection\n",
        "        collection_rolling = collection_group[['price', 'volume']].shift(1).rolling(window=window)\n",
        "        features.update({\n",
        "            f'collection_price_roll_mean_{window}': collection_rolling['price'].mean(),\n",
        "            f'collection_price_roll_std_{window}': collection_rolling['price'].std(),\n",
        "            f'collection_price_roll_var_{window}': collection_rolling['price'].var(),\n",
        "            f'collection_price_roll_sum_{window}': collection_rolling['price'].sum(),\n",
        "        })\n",
        "      else:\n",
        "        features.update({\n",
        "            f'collection_price_roll_mean_{window}': np.nan,\n",
        "            f'collection_price_roll_std_{window}': np.nan,\n",
        "            f'collection_price_roll_var_{window}': np.nan,\n",
        "            f'collection_price_roll_sum_{window}': np.nan,\n",
        "        })\n",
        "\n",
        "    # Średnia cena i wolumen dla całej kolekcji (tylko z przeszłych rekordów)\n",
        "    collection_mean = collection_group[['price', 'volume']].shift(1).expanding().mean()\n",
        "    features['collection_price_mean'] = collection_mean['price']\n",
        "    features['collection_vol_mean'] = collection_mean['volume']\n",
        "\n",
        "    return pd.concat([group, pd.DataFrame(features, index=group.index)], axis=1)\n",
        "\n",
        "  dataset = dataset.groupby('name', group_keys=False).apply(compute_features)\n",
        "\n",
        "  dates = dataset.index.get_level_values('date')\n",
        "  dataset['year'] = dates.year\n",
        "  dataset['month'] = dates.month\n",
        "  dataset['day'] = dates.day\n",
        "  dataset['weekday'] = dates.weekday\n",
        "  #dataset['is_weekend'] = (dataset['weekday'] >= 5)\n",
        "  dataset['d_from_nyear'] = (dates - pd.to_datetime(dates.year.astype(str) + '-01-01')).days\n",
        "  dataset['d_to_june'] = (pd.to_datetime(dates.year.astype(str) + '-06-01') - dates).days\n",
        "\n",
        "\n",
        "  dataset[bool_columns] = dataset[bool_columns].astype(bool)\n",
        "  #int_columns_present = [col for col in int_columns if col in dataset.columns]\n",
        "  dataset[int_columns] = dataset[int_columns].fillna(0).astype(int)\n",
        "  dataset[categorical_columns] = dataset[categorical_columns].astype('category')\n",
        "\n",
        "  dataset.dropna(subset=['itemType'], inplace=True)  # Drop interpolated rows, subset is whatever is not interpolated\n",
        "\n",
        "  return dataset\n",
        "\n",
        "dp.add_features = add_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "id": "Vgyp8dG4fkAe",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Split\n",
        "\"\"\"dataset = dp.dataset.copy()\n",
        "dataset.set_index(['name','date'],inplace=True)\n",
        "dataset = dp.add_features(dataset)\n",
        "\n",
        "X_whole = dataset.drop(columns=['price'])\n",
        "y_whole = dataset['price'].copy()\n",
        "\n",
        "X_train, X_test, y_train, y_test = split_dataset(X_whole,y_whole)\n",
        "\n",
        "train_lgb_dataset = lgb.Dataset(X_train, label=y_train,free_raw_data=0)\n",
        "test_lgb_dataset = lgb.Dataset(X_test, label=y_test,free_raw_data=0)\n",
        "\n",
        "# For convenience:\n",
        "train_dataset=train_lgb_dataset.data.copy()       # Same jak bym dał do tego X_train i y_train chyba?\n",
        "train_dataset.loc[:,'price']=train_lgb_dataset.label\n",
        "\n",
        "test_dataset=test_lgb_dataset.data.copy()\n",
        "test_dataset.loc[:,'price']=test_lgb_dataset.label\"\"\"\n",
        "pass\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title params\n",
        "const_params={\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'mae',\n",
        "    'metric': ['mape','mae'],\n",
        "\n",
        "    'seed': 42,\n",
        "    'num_threads':4,   # 5K dart rounds x3, 4 threads are the best, 30% faster than 8 or -1 and 8% faster than second fastest number score\n",
        "    'device': 'gpu' if gpu_available else 'cpu',\n",
        "    'gpu_platform_id': 0,\n",
        "    'gpu_device_id': 0,\n",
        "    'verbose': 1,\n",
        "\n",
        "    'learning_rate': 0.1,\n",
        "    'num_boost_round': 500#1500\n",
        "    }\n",
        "\n",
        "model_params=const_params.copy()\n",
        "if TEST_MODE:\n",
        "  model_params.update({\n",
        "      'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 10,\n",
        "      'lambda_l1': 15, 'lambda_l2': 15,\n",
        "      'min_data_in_leaf': 5, 'min_gain_to_split': 0.1, 'max_depth': 6,\n",
        "      'num_leaves': 12, 'max_bin':400,\n",
        "  })\n",
        "else:\n",
        "  model_params.update({\n",
        "      'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 20,\n",
        "      'lambda_l1': 50, 'lambda_l2': 5,\n",
        "      'min_data_in_leaf': 30, 'min_gain_to_split': 2, 'max_depth': 9,\n",
        "      'num_leaves': 80, 'max_bin':512,\n",
        "  })"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3BQWoOSO4kqB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title add features, split test dataset\n",
        "dataset = dp.dataset.copy()\n",
        "dataset.set_index(['name', 'date'], inplace=True)\n",
        "dataset = dp.add_features(dataset)\n",
        "\n",
        "_features_added=True\n",
        "\n",
        "\n",
        "X_whole = dataset.drop(columns=['price'])\n",
        "y_whole = dataset['price'].copy()\n",
        "\n",
        "X, X_test, y, y_test = split_dataset(X_whole,y_whole,test_size=0.15)\n",
        "\n",
        "#X, X_tmp, y, y_tmp = split_dataset(X_whole,y_whole,test_size=0.3)\n",
        "#X_valid, X_test, y_valid, y_test = split_dataset(X_whole,y_whole,test_size=0.3)\n",
        "\n",
        "del X_whole,y_whole\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "u3nV2luI0iLn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Learning with cv\n",
        "def learnCV(X,y,n_splits):\n",
        "  all_y_pred_train,all_y_train = [],[]\n",
        "  all_y_pred_test,all_y_test = [],[]\n",
        "  all_y_pred_naive = []\n",
        "  tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "  for train_index, test_index in tscv.split(X):\n",
        "      X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "      y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "      train_lgb_dataset = lgb.Dataset(X_train, label=y_train, free_raw_data=False)\n",
        "      test_lgb_dataset = lgb.Dataset(X_test, label=y_test, free_raw_data=False)\n",
        "\n",
        "      model = lgb.train(\n",
        "          model_params,\n",
        "          train_lgb_dataset,\n",
        "          valid_sets=[test_lgb_dataset],\n",
        "          valid_names=['test'],\n",
        "          callbacks=[\n",
        "              lgb.log_evaluation(period=50),\n",
        "              lgb.early_stopping(stopping_rounds=100, min_delta=[0.0008,0.001])\n",
        "          ]\n",
        "      )\n",
        "\n",
        "      y_pred_train = pd.Series(model.predict(X_train, num_iteration=model.best_iteration), index=X_train.index)\n",
        "      y_pred_test = pd.Series(model.predict(X_test, num_iteration=model.best_iteration), index=X_test.index)\n",
        "\n",
        "      y_pred_naive = pd.Series(X_test['price_lag2'].fillna(X_test['storePrice']).fillna(3).to_numpy(), index=X_test.index)\n",
        "\n",
        "      all_y_pred_train.append(y_pred_train)\n",
        "      all_y_train.append(y_train)\n",
        "      all_y_pred_test.append(y_pred_test)\n",
        "      all_y_test.append(y_test)\n",
        "      all_y_pred_naive.append(y_pred_naive)\n",
        "\n",
        "  del X_train, X_test,y_train, y_test,train_lgb_dataset,test_lgb_dataset, y_pred_train,y_pred_test,y_pred_naive\n",
        "  gc.collect()\n",
        "\n",
        "  y_pred_train_concat = pd.concat(all_y_pred_train)\n",
        "  y_train_concat = pd.concat(all_y_train)\n",
        "  y_pred_test_concat = pd.concat(all_y_pred_test)\n",
        "  y_test_concat = pd.concat(all_y_test)\n",
        "  y_pred_naive_concat = pd.concat(all_y_pred_naive)\n",
        "  return model,(y_pred_train_concat, y_train_concat, y_pred_test_concat, y_test_concat, y_pred_naive_concat)\n",
        "\n",
        "\n",
        "n_splits = 5\n",
        "\n",
        "gc.collect()\n",
        "memory = psutil.virtual_memory().used\n",
        "\n",
        "model,datasets = learnCV(X, y, n_splits)\n",
        "\n",
        "memory2 = psutil.virtual_memory().used\n",
        "\n",
        "print(f\"RAM difference: {(memory2 -memory ) / (1024 ** 2):.2f} MB\")\n",
        "\n",
        "\n",
        "if not TEST_MODE:\n",
        "  autosave_model(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "cellView": "form",
        "id": "_98q2TI50iSV",
        "outputId": "db8ccc0b-530e-4ae4-da71-3c57a3b07f5d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:204: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002341 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 11989\n",
            "[LightGBM] [Info] Number of data points in the train set: 875, number of used features: 73\n",
            "[LightGBM] [Info] Start training from score 1.450000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[50]\ttest's mape: 0.109237\ttest's l1: 0.223157\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[100]\ttest's mape: 0.108951\ttest's l1: 0.219632\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[150]\ttest's mape: 0.107777\ttest's l1: 0.215614\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[200]\ttest's mape: 0.107777\ttest's l1: 0.215614\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Early stopping, best iteration is:\n",
            "[129]\ttest's mape: 0.108001\ttest's l1: 0.215953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:204: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004277 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 18703\n",
            "[LightGBM] [Info] Number of data points in the train set: 1745, number of used features: 73\n",
            "[LightGBM] [Info] Start training from score 1.420000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[50]\ttest's mape: 0.127739\ttest's l1: 0.375524\n",
            "[100]\ttest's mape: 0.12329\ttest's l1: 0.358573\n",
            "[150]\ttest's mape: 0.122473\ttest's l1: 0.354141\n",
            "[200]\ttest's mape: 0.121467\ttest's l1: 0.347381\n",
            "[250]\ttest's mape: 0.120292\ttest's l1: 0.341895\n",
            "[300]\ttest's mape: 0.119736\ttest's l1: 0.337977\n",
            "[350]\ttest's mape: 0.119824\ttest's l1: 0.337186\n",
            "Early stopping, best iteration is:\n",
            "[263]\ttest's mape: 0.119346\ttest's l1: 0.337709\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003686 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 21239\n",
            "[LightGBM] [Info] Number of data points in the train set: 2615, number of used features: 74\n",
            "[LightGBM] [Info] Start training from score 1.640000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Training until validation scores don't improve for 100 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:204: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\ttest's mape: 0.0822302\ttest's l1: 0.280382\n",
            "[100]\ttest's mape: 0.0750563\ttest's l1: 0.248934\n",
            "[150]\ttest's mape: 0.0751353\ttest's l1: 0.249537\n",
            "Early stopping, best iteration is:\n",
            "[69]\ttest's mape: 0.0753327\ttest's l1: 0.249778\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006706 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24763\n",
            "[LightGBM] [Info] Number of data points in the train set: 3485, number of used features: 74\n",
            "[LightGBM] [Info] Start training from score 1.950000\n",
            "Training until validation scores don't improve for 100 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:204: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\ttest's mape: 0.0928533\ttest's l1: 0.212624\n",
            "[100]\ttest's mape: 0.0929158\ttest's l1: 0.21239\n",
            "Early stopping, best iteration is:\n",
            "[37]\ttest's mape: 0.0931593\ttest's l1: 0.214737\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007419 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24858\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:204: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of data points in the train set: 4355, number of used features: 74\n",
            "[LightGBM] [Info] Start training from score 2.120000\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[50]\ttest's mape: 0.0974933\ttest's l1: 0.197083\n",
            "[100]\ttest's mape: 0.0970719\ttest's l1: 0.191046\n",
            "[150]\ttest's mape: 0.0972349\ttest's l1: 0.189888\n",
            "Early stopping, best iteration is:\n",
            "[64]\ttest's mape: 0.096826\ttest's l1: 0.193105\n",
            "RAM difference: 281.90 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_metrics(*datasets, label=\"mae\")\n",
        "y_pred_train, y_pred_valid, y_pred_naive = datasets[0],datasets[2],datasets[-1]\n",
        "sum=0\n",
        "for d in datasets:\n",
        "  sum+=getsizeof(y_pred_train)\n",
        "print(round(sum/1024/1024,2),\"MB\")\n",
        "round(getsizeof(y)/1024/1024,2)\n",
        "#848MB predicted size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "bpTYEKFx0iY-",
        "outputId": "8b91029f-2b00-4c78-958c-d3ec2c7e7527"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== mae ========\n",
            "RMSE change: -0.56%   (naive->test)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                  Metric   Train   Valid   Naive\n",
              "0               Accuracy  93.010  90.059  89.518\n",
              "1                   RMSE   0.335   0.425   0.428\n",
              "2                    MAE   0.149   0.242   0.235\n",
              "3                   MAPE   0.070   0.099   0.105\n",
              "4              Max Error   7.145   7.249   7.520\n",
              "5  Median Absolute Error   0.069   0.138   0.140"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-240f14d3-3fa2-4c26-bdf7-ea0473ae2dd1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>Train</th>\n",
              "      <th>Valid</th>\n",
              "      <th>Naive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Accuracy</td>\n",
              "      <td>93.010</td>\n",
              "      <td>90.059</td>\n",
              "      <td>89.518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RMSE</td>\n",
              "      <td>0.335</td>\n",
              "      <td>0.425</td>\n",
              "      <td>0.428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>MAE</td>\n",
              "      <td>0.149</td>\n",
              "      <td>0.242</td>\n",
              "      <td>0.235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MAPE</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Max Error</td>\n",
              "      <td>7.145</td>\n",
              "      <td>7.249</td>\n",
              "      <td>7.520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Median Absolute Error</td>\n",
              "      <td>0.069</td>\n",
              "      <td>0.138</td>\n",
              "      <td>0.140</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-240f14d3-3fa2-4c26-bdf7-ea0473ae2dd1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-240f14d3-3fa2-4c26-bdf7-ea0473ae2dd1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-240f14d3-3fa2-4c26-bdf7-ea0473ae2dd1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ce7341b0-93e8-48f1-9bbd-feaad83174f7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ce7341b0-93e8-48f1-9bbd-feaad83174f7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ce7341b0-93e8-48f1-9bbd-feaad83174f7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"#848MB predicted size\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"Metric\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Accuracy\",\n          \"RMSE\",\n          \"Median Absolute Error\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Train\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 37.44156602316023,\n        \"min\": 0.069,\n        \"max\": 93.01,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          93.01,\n          0.335,\n          0.069\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Valid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 36.210053922449035,\n        \"min\": 0.099,\n        \"max\": 90.059,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          90.059,\n          0.425,\n          0.138\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Naive\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 35.97607150685948,\n        \"min\": 0.105,\n        \"max\": 89.518,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          89.518,\n          0.428,\n          0.14\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.15 MB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.15"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "bXZ342njGDQW"
      },
      "outputs": [],
      "source": [
        "#@title Utils\n",
        "\"\"\"test_names = X_test.index.get_level_values('name').unique()\n",
        "train_names = X_train.index.get_level_values('name').unique()\n",
        "all_names = test_names.union(train_names)\n",
        "unique_dataset = dp.dataset.loc[dp.dataset.groupby('name')['price'].idxmax()]\n",
        "sorted_dataset_price = unique_dataset.sort_values(by='price', ascending=False)\"\"\"\n",
        "#display(sorted_dataset_price.head(5))\n",
        "pass\n",
        "\n",
        "#pd.set_option('display.max_columns', None)\n",
        "#print(\"\\nTest most freq:\")\n",
        "#display(X_test.groupby('name').size().sort_values(ascending=False).head(5))\n",
        "#print(\"\\nTrain most freq:\")\n",
        "#display(X_train.groupby('name').size().sort_values(ascending=False).head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "DVVzjzS2lQad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20c22538-217f-4011-ec0e-8fe3329e3e2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Labyrinth Door']\n",
            "Labyrinth Door  not in dataset\n"
          ]
        }
      ],
      "source": [
        "#@title Display pricehistories\n",
        "names=['Labyrinth Door']\n",
        "#names=[n for n in all_names if \"Whiteout\" in n]\n",
        "#names=['Legacy Kevlar Helmet','Whiteout Helmet']\n",
        "print(names)\n",
        "\n",
        "phs=[]\n",
        "end_names=[]\n",
        "for name in names:\n",
        "  try:\n",
        "    ph = dp.dataset.loc[name][['price']]\n",
        "  except KeyError:\n",
        "    print(name,\" not in dataset\")\n",
        "    continue\n",
        "  phs.append(ph)\n",
        "  end_names.append(name)\n",
        "  #pricehistory2 = test_dataset.loc[name][['price']]\n",
        "if len(phs):\n",
        "  plot_pricehistory(phs,end_names,\"test\")\n",
        "  plot_pricehistory(phs,end_names,\"test\",relative_x_axis=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cdh32rU8v0mk",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Grid search results plotter\n",
        "from collections import defaultdict, Counter\n",
        "from itertools import combinations\n",
        "\n",
        "def show_grid_search_results(gsearch, top_frac=0.2, dp_styled_res=False,show_overall_heatmap=True):\n",
        "    results = pd.DataFrame(gsearch.cv_results_)\n",
        "    selected_columns = ['params', 'mean_test_score', 'std_test_score', 'rank_test_score']\n",
        "    results = results[selected_columns].sort_values(by='rank_test_score')\n",
        "    styled_results = results.style.background_gradient(subset=['rank_test_score'], cmap='coolwarm')\n",
        "    styled_results = styled_results.format({'mean_test_score': '{:.3f}', 'std_test_score': '{:.3f}'})\n",
        "\n",
        "    if dp_styled_res:\n",
        "        display(styled_results)\n",
        "\n",
        "    top_records_num = int(len(results) * top_frac)\n",
        "    top_results = results.nsmallest(top_records_num, 'rank_test_score')\n",
        "\n",
        "    # Count parameter occurrences\n",
        "    param_counts = Counter()\n",
        "    for params in top_results['params']:\n",
        "        for param, value in params.items():\n",
        "            param_key = f\"{param}={value}\"\n",
        "            param_counts[param_key] += 1\n",
        "\n",
        "    # Filter out parameters that occur in all top records\n",
        "    filtered_params = {k: v for k, v in param_counts.items() if v != top_records_num}\n",
        "\n",
        "    # Convert Counter to DataFrame\n",
        "    param_counts_df = pd.DataFrame.from_dict(filtered_params, orient='index', columns=['frequency']).sort_values('frequency', ascending=False)\n",
        "\n",
        "    # Extract unique parameter names\n",
        "    param_labels = param_counts_df.index\n",
        "    param_names = [label.split('=')[0] for label in param_labels]\n",
        "    unique_params = list(set(param_names))\n",
        "\n",
        "    # Assign colors to unique parameter names\n",
        "    colors = plt.cm.get_cmap('tab10', len(unique_params))\n",
        "    color_map = {param: colors(i) for i, param in enumerate(unique_params)}\n",
        "\n",
        "    # Plot bar chart of parameter frequencies\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    bars = ax.bar(param_counts_df.index, param_counts_df['frequency'], color=[color_map[param] for param in param_names])\n",
        "    ax.set_title(f'Częstość argumentów w {top_records_num}/{len(results)} ({100*top_frac:.0f}%) najlepszych wynikach')\n",
        "    ax.set_xlabel('Parametry')\n",
        "    ax.set_ylabel('Częstość')\n",
        "    ax.set_xticks(np.arange(len(param_labels)))\n",
        "    ax.set_xticklabels(param_labels, rotation=45, ha='right', fontsize=10, color='black')\n",
        "\n",
        "    for tick, param_name in zip(ax.get_xticklabels(), param_names):\n",
        "        tick.set_color(color_map[param_name])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    if show_overall_heatmap:\n",
        "      param_correlations = defaultdict(int)\n",
        "      for params in top_results['params']:\n",
        "          param_items = [f\"{param}={value}\" for param, value in params.items()]\n",
        "          for combo in combinations(param_items, 2):\n",
        "              param_correlations[combo] += 1\n",
        "\n",
        "      correlation_matrix = pd.DataFrame(0, index=param_labels, columns=param_labels)\n",
        "      for (param1, param2), count in param_correlations.items():\n",
        "          correlation_matrix.loc[param1, param2] = count\n",
        "          correlation_matrix.loc[param2, param1] = count\n",
        "\n",
        "      correlation_matrix = correlation_matrix.loc[param_counts_df.index, param_counts_df.index]\n",
        "      plt.figure(figsize=(9, 8))\n",
        "      sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.0f', cbar=True, linewidths=0.5)\n",
        "      plt.title('Częstości występowania par parametrów')\n",
        "      plt.xlabel('Parametry')\n",
        "      plt.ylabel('Parametry')\n",
        "      plt.xticks(rotation=45, ha='right')\n",
        "      plt.yticks(rotation=0)\n",
        "      for tick, param_name in zip(plt.gca().get_xticklabels(), param_names):\n",
        "          tick.set_color(color_map[param_name])\n",
        "      for tick, param_name in zip(plt.gca().get_yticklabels(), param_names):\n",
        "          tick.set_color(color_map[param_name])\n",
        "      plt.tight_layout()\n",
        "      plt.show()\n",
        "\n",
        "    # Individual heatmaps for each pair of parameter types in filtered data\n",
        "    filtered_param_names = list(set(param_names))\n",
        "\n",
        "    for param1, param2 in combinations(filtered_param_names, 2):\n",
        "        param1_values = sorted({params.get(param1) for params in top_results['params'] if param1 in params})\n",
        "        param2_values = sorted({params.get(param2) for params in top_results['params'] if param2 in params})\n",
        "\n",
        "        correlation_matrix = pd.DataFrame(0, index=param1_values, columns=param2_values)\n",
        "\n",
        "        for params in top_results['params']:\n",
        "            if param1 in params and param2 in params:\n",
        "                correlation_matrix.loc[params[param1], params[param2]] += 1\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.0f', cbar=True, linewidths=0.5)\n",
        "        plt.title(f'Heatmap for {param1} vs {param2}')\n",
        "        plt.xlabel(param2)\n",
        "        plt.ylabel(param1)\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.yticks(rotation=0)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YBq7p3-6JqJ",
        "outputId": "c4f3043f-61a5-4701-880b-ce759f590234",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 4 folds for each of 24 candidates, totalling 96 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:204: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found by grid search are: {'bagging_fraction': 0.9, 'bagging_freq': 20, 'feature_fraction': 0.8, 'lambda_l1': 15, 'lambda_l2': 15, 'learning_rate': 0.1, 'max_depth': 9, 'min_data_in_leaf': 30, 'min_gain_to_split': 0.1, 'num_boost_round': 200, 'num_leaves': 80, 'verbose': -1}\n",
            "Best score found by grid search is: 0.08808318078513527\n",
            "Czas wykonania: 1:05:39.645659\n"
          ]
        }
      ],
      "source": [
        "#@title Grid search\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "# Uzyskanie bieżącego czasu\n",
        "start_time = datetime.now()\n",
        "\n",
        "def hyperparameter_optimization(X, y):\n",
        "    lgb_estimator = lgb.LGBMRegressor(\n",
        "        objective='mae',\n",
        "        boosting_type='gbdt',\n",
        "        metric='mape',\n",
        "        device='gpu' if gpu_available else 'cpu',\n",
        "        gpu_platform_id=0,\n",
        "        gpu_device_id=0,\n",
        "        seed=42,\n",
        "        num_threads=4,\n",
        "        verbose=-1,\n",
        "        learning_rate=0.1,\n",
        "    )\n",
        "\n",
        "    param_grid = {\n",
        "        'feature_fraction': [0.9,0.8,0.7],#[0.9,0.5],\n",
        "        'bagging_fraction': [0.9],\n",
        "        'bagging_freq': [20,25],\n",
        "        'verbose':[-1],\n",
        "        'min_data_in_leaf': [30],\n",
        "        'min_gain_to_split': [0.1],\n",
        "        'max_depth': [6,7,8,9],\n",
        "        'lambda_l1': [15],#[10,15],\n",
        "        'lambda_l2': [15],#[10,15],\n",
        "        'learning_rate': [0.1],\n",
        "        'num_leaves': [80],\n",
        "        'num_boost_round': [200],\n",
        "    }\n",
        "\n",
        "    lgb_estimator = lgb.LGBMRegressor()\n",
        "\n",
        "    gsearch = GridSearchCV(estimator=lgb_estimator, param_grid=param_grid, cv=4,verbose=1, n_jobs=2, scoring='neg_median_absolute_error')#neg_mean_absolute_error\n",
        "    gsearch.fit(X, y)\n",
        "\n",
        "    print(\"Best parameters found by grid search are:\", gsearch.best_params_)\n",
        "    print(\"Best score found by grid search is:\", -gsearch.best_score_)\n",
        "\n",
        "    return gsearch\n",
        "\n",
        "gsearch1 = hyperparameter_optimization(X_train, y_train)\n",
        "\n",
        "#0.149 'feature_fraction': 0.6,  'max_depth': -1,  'min_gain_to_split': 0.1, 'num_leaves': 12,\n",
        "# 0.143  'feature_fraction': 0.9, 'max_bin': 512, 'min_gain_to_split': 0.1, 'num_boost_round': 300, 'num_leaves': 26,\n",
        "#'max_bin': 400, 'max_depth': 6, 'min_data_in_leaf': 40, 'min_gain_to_split': 0.1, 'num_leaves': 26,\n",
        "#'max_bin': 512, 'max_depth': -1, 'min_data_in_leaf': 40, 'min_gain_to_split': 0.1,  'num_leaves': 26,\n",
        "\n",
        "#cv=5\n",
        "# 'max_bin': 400, 'max_depth': 10, 'min_data_in_leaf': 60, 'min_gain_to_split': 0.1, 'num_leaves': 26,\n",
        "# 0.117\n",
        "\n",
        "#mean: 0.14340med 0.211mean\n",
        "#med:  0.1434med  0.212mean\n",
        "print(\"Czas wykonania:\", datetime.now()-start_time)\n",
        "\n",
        "#0.15median 'bagging_freq': 10, 'lambda_l1': 1, 'lambda_l2': 1, 'max_depth': -1, 'min_data_in_leaf': 40, 'min_gain_to_split': 0.1, 'num_leaves': 9\n",
        "#0.22mean   'bagging_freq': 10,  'lambda_l1': 1, 'lambda_l2': 1, 'max_depth': -1, 'min_data_in_leaf': 40, 'min_gain_to_split': 0.1,  'num_leaves': 9,\n",
        "\n",
        "\n",
        "\n",
        "#0.256 sredni czas na 2foldy train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "n1ttn2nr6DUZ",
        "outputId": "73d6829a-434b-409e-ec40-103a0587bc51"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'show_grid_search_results' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e48e3d92e307>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_grid_search_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgsearch1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'show_grid_search_results' is not defined"
          ]
        }
      ],
      "source": [
        "show_grid_search_results(gsearch1,0.3,False,True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEsvNX7Tsa80"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AsXYxeHWGqXF",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title params & learn\n",
        "const_params={\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'mae',\n",
        "    'metric': ['mape','mae'],\n",
        "    #'min_delta':[0.001,0.001],\n",
        "    #'early_stopping_rounds':200,\n",
        "    #'first_metric_only': \"true\",\n",
        "\n",
        "\n",
        "    'seed': 42,\n",
        "    'num_threads':4,   # 5K dart rounds x3, 4 threads are the best, 30% faster than 8 or -1 and 8% faster than second fastest number score\n",
        "    'device': 'gpu' if gpu_available else 'cpu',\n",
        "    'gpu_platform_id': 0,\n",
        "    'gpu_device_id': 0,\n",
        "    'verbose': 1,\n",
        "\n",
        "    'learning_rate': 0.1,\n",
        "    'num_boost_round': 500#1500\n",
        "    }\n",
        "\n",
        "model_params=const_params.copy()\n",
        "if TEST_MODE:\n",
        "  model_params.update({\n",
        "      'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 10,\n",
        "      'lambda_l1': 15, 'lambda_l2': 15,\n",
        "      'min_data_in_leaf': 5, 'min_gain_to_split': 0.1, 'max_depth': 6,\n",
        "      'num_leaves': 12, 'max_bin':400,\n",
        "  })\n",
        "else:\n",
        "  model_params.update({\n",
        "      'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 20,\n",
        "      'lambda_l1': 50, 'lambda_l2': 5,\n",
        "      'min_data_in_leaf': 30, 'min_gain_to_split': 2, 'max_depth': 9,\n",
        "      'num_leaves': 80, 'max_bin':512,\n",
        "  })\n",
        "\n",
        "model = lgb.train(model_params,\n",
        "                  train_lgb_dataset,\n",
        "                  valid_sets=[test_lgb_dataset],\n",
        "                  valid_names=['test'],\n",
        "                  callbacks=[lgb.log_evaluation(period=50),\n",
        "                             lgb.early_stopping(stopping_rounds=100,min_delta=[0.0008,0.001],first_metric_only=False)\n",
        "                             ])  #200 rounds:  L4 71s | T4 87s | CPU 110s\n",
        "\n",
        "if not TEST_MODE:\n",
        "  folder_path='/content/drive/MyDrive/SMF_files/model_saves'\n",
        "  curr_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "  file_path = os.path.join(folder_path, f\"lightgbm_model_{curr_time}.txt\")\n",
        "  model.save_model(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "collapsed": true,
        "id": "YDmpWOq2gSxm",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Metrics\n",
        "\"\"\"def show_metrics_standard(model, X_train, y_train, X_test, y_test, label=\"[no label]\"):\n",
        "    # Pobranie najlepszej liczby iteracji\n",
        "    num_iteration = model.best_iteration\n",
        "\n",
        "    print('='*8+\" \"+label+\" \"+'='*8)\n",
        "\n",
        "    # Przewidywania za pomocą metody naiwnej\n",
        "    y_pred_naive = pd.Series(X_test['price_lag2'].fillna(X_test['storePrice']).fillna(3).to_numpy(), index=X_test.index)\n",
        "    rmse_naive = mean_squared_error(y_test, y_pred_naive, squared=False)\n",
        "\n",
        "    # Przewidywania na danych treningowych\n",
        "    y_pred_train = pd.Series(model.predict(X_train, num_iteration=num_iteration), index=X_train.index)\n",
        "    rmse_train = mean_squared_error(y_train, y_pred_train, squared=False)\n",
        "\n",
        "    # Przewidywania na danych testowych\n",
        "    y_pred = pd.Series(model.predict(X_test, num_iteration=num_iteration), index=X_test.index)\n",
        "    rmse_test = mean_squared_error(y_test, y_pred, squared=False)\n",
        "\n",
        "    # Wyświetlanie zmian RMSE w porównaniu do metody naiwnej\n",
        "    print(f'RMSE change: {(1-rmse_test/rmse_naive)*-100:.2f}%   (naive->test)')\n",
        "\n",
        "    # Wyświetlanie metryk\n",
        "    results = {\n",
        "        \"Metric\": [\"Accuracy\", \"RMSE\", \"MAE\", \"MAPE\", \"Max Error\", \"Median Absolute Error\"],\n",
        "        \"Train\": calc_accuracy(y_pred_train, y_train, \"Train accuracy:\"),\n",
        "        \"Valid\": calc_accuracy(y_pred, y_test, \"Validation accuracy:\"),\n",
        "        \"Naive\": calc_accuracy(y_pred_naive, y_test, \"Naive accuracy:\")\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    display(df)\n",
        "\n",
        "    return y_pred, y_pred_train, y_pred_naive\n",
        "\n",
        "y_pred, y_pred_train, y_pred_naive = show_metrics_standard(model, X_train, y_train, X_test, y_test, label=\"mae\")\n",
        "\n",
        "#y_pred,y_pred_train,y_pred_naive=show_metrics(model,\"mae\")\n",
        "#y_pred2,y_pred_train2,_ = show_metrics(model2,\"Model 2 reg_l1\")\"\"\"\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title N-models params\n",
        "const_params_n={\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'mae',\n",
        "    'metric': ['mape','mae'],\n",
        "\n",
        "    'seed': 42,\n",
        "    'num_threads':4,   # 5K dart rounds x3, 4 threads are the best, 30% faster than 8 or -1 and 8% faster than second fastest number score\n",
        "    'device': 'gpu' if gpu_available else 'cpu',\n",
        "    'gpu_platform_id': 0,\n",
        "    'gpu_device_id': 0,\n",
        "    'verbose': 1,\n",
        "\n",
        "    'learning_rate': 0.1,\n",
        "    'num_boost_round': 500\n",
        "    }\n",
        "\n",
        "model_params_n=const_params_n.copy()\n",
        "if TEST_MODE:\n",
        "  model_params_n.update({\n",
        "      'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 10,\n",
        "      'lambda_l1': 15, 'lambda_l2': 15,\n",
        "      'min_data_in_leaf': 5, 'min_gain_to_split': 0.1, 'max_depth': 6,\n",
        "      'num_leaves': 12, 'max_bin':400,\n",
        "  })\n",
        "else:\n",
        "  model_params_n.update({\n",
        "      'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 20,\n",
        "      'lambda_l1': 50, 'lambda_l2': 5,\n",
        "      'min_data_in_leaf': 30, 'min_gain_to_split': 2, 'max_depth': 9,\n",
        "      'num_leaves': 80, 'max_bin':512,\n",
        "  })"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vdDXrDMl2obA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "98vLokLZjsAT",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title N-models old\n",
        "'''\n",
        "def add_shifted_targets(dataset, max_n):\n",
        "    \"\"\"\n",
        "    Tworzy zbiór danych z przesuniętymi targetami dla różnych horyzontów prognozy.\n",
        "    \"\"\"\n",
        "    target_columns = []\n",
        "    for n in range(1, max_n + 1):\n",
        "        dataset[f'price_t+n_{n}'] = dataset.groupby('name')['price'].shift(-n)\n",
        "        target_columns.append(f'price_t+n_{n}')\n",
        "\n",
        "    #dataset.drop(columns=['price'],inplace=True)\n",
        "    #dataset.rename(columns={'price': 'price_lag0'}, inplace=True)\n",
        "    return dataset, target_columns\n",
        "\n",
        "def train_models_for_forecast_horizon(X_train, ys_train, X_test, ys_test, model_params):\n",
        "    \"\"\"\n",
        "    Trenuje modele dla różnych horyzontów prognozy.\n",
        "    \"\"\"\n",
        "    models = {}\n",
        "    for i, y_col in enumerate(ys_train):\n",
        "        y_train_tmp = ys_train[y_col].copy().dropna()\n",
        "        X_train_tmp = X_train.loc[y_train_tmp.index]#.drop(columns=ys_train)\n",
        "\n",
        "        y_test_tmp = ys_test[y_col].copy().dropna()\n",
        "        X_test_tmp = X_test.loc[y_test_tmp.index]#.drop(columns=y_test_tmp)\n",
        "\n",
        "        train_lgb_dataset = lgb.Dataset(X_train_tmp, label=y_train_tmp, free_raw_data=1)\n",
        "        test_lgb_dataset = lgb.Dataset(X_test_tmp, label=y_test_tmp, free_raw_data=1)\n",
        "\n",
        "        model = lgb.train(model_params, train_lgb_dataset,\n",
        "                          valid_sets=[test_lgb_dataset],\n",
        "                          valid_names=[y_col],\n",
        "                          callbacks=[lgb.log_evaluation(period=50),\n",
        "                                     lgb.early_stopping(stopping_rounds=100,min_delta=[0.001,0.001],first_metric_only=False)\n",
        "                                     ])\n",
        "        del train_lgb_dataset, test_lgb_dataset, X_train_tmp, y_train_tmp, X_test_tmp, y_test_tmp\n",
        "        gc.collect()\n",
        "\n",
        "        #model.save_model(f'model_horizon_{i+1}.txt')\n",
        "        models[f'model_horizon_{i+1}'] = model\n",
        "        del model\n",
        "        print(f'Model for horizon {i+1} trained.')\n",
        "\n",
        "    return models\n",
        "\n",
        "def show_metrics(models, X_test, ys_test):\n",
        "\n",
        "    #Wyświetla dokładność modeli dla poszczególnych horyzontów prognozy oraz średnią dokładność.\n",
        "\n",
        "    metrics = {'mape': [], 'mae': [], 'rmse': []}\n",
        "\n",
        "    for i, y_col in enumerate(ys_test):\n",
        "\n",
        "        y_test_tmp = ys_test[y_col].dropna()\n",
        "        X_test_tmp = X_test.loc[y_test_tmp.index]#.drop(columns=y_columns)\n",
        "\n",
        "        preds = models[f'model_horizon_{i+1}'].predict(X_test_tmp)\n",
        "\n",
        "        mape = mean_absolute_percentage_error(y_test_tmp, preds)\n",
        "        mae = mean_absolute_error(y_test_tmp, preds)\n",
        "        rmse = np.sqrt(mean_squared_error(y_test_tmp, preds))\n",
        "\n",
        "        metrics['mape'].append(mape)\n",
        "        metrics['mae'].append(mae)\n",
        "        metrics['rmse'].append(rmse)\n",
        "\n",
        "        print(f'Metrics for horizon {i+1}: MAPE: {mape:.4f}, MAE: {mae:.4f}, RMSE: {rmse:.4f}')\n",
        "\n",
        "    for metric in metrics:\n",
        "        avg_metric = np.mean(metrics[metric])\n",
        "        print(f'Average {metric.upper()}: {avg_metric:.4f}')\n",
        "\n",
        "# Tworzenie targetów dla N dni do przodu\n",
        "N = 7\n",
        "dataset_n = dp.dataset.copy()\n",
        "dataset_n.set_index(['name', 'date'], inplace=True)\n",
        "dataset_n = dp.add_features(dataset_n)\n",
        "\n",
        "dataset_n, target_columns_n = add_shifted_targets(dataset_n, N)\n",
        "\n",
        "common_idx = dataset_n.index #dataset_n[target_columns_n].dropna().index\n",
        "display(dataset_n.head())\n",
        "\n",
        "X_filtered = dataset_n.loc[common_idx].drop(columns=target_columns_n)\n",
        "y_filtered = dataset_n[target_columns_n].loc[common_idx]\n",
        "\n",
        "X_train_n, X_test_n, y_train_n, y_test_n = split_dataset(X_filtered, y_filtered)\n",
        "\n",
        "#display(X_test_n.head())\n",
        "#display(y_test_n.head())\n",
        "\n",
        "models = train_models_for_forecast_horizon(X_train_n, y_train_n, X_test_n, y_test_n, model_params_n)\n",
        "\n",
        "show_metrics(models, X_test_n, y_test_n)\n",
        "'''\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title eval_models_n\n",
        "\n",
        "def get_test_index_TSS(X,n_splits):\n",
        "  \"\"\" Last test index from TimeSeriesSplit was never used for learning, only for validation.\n",
        "      Returns that test index.\"\"\"\n",
        "  tscv = TimeSeriesSplit(n_splits_n)\n",
        "  tscv.split(X)\n",
        "  test_index=None\n",
        "  for train_index_tmp, test_index_tmp in tscv.split(X):\n",
        "      test_index=test_index_tmp\n",
        "  return test_index\n",
        "\n",
        "def eval_models_n(models, X_test, ys_test):\n",
        "\n",
        "    df = pd.DataFrame(columns=[ 'MAPE', 'MAE', 'RMSE', 'Count'])\n",
        "\n",
        "    for i, y_col in enumerate(ys_test):\n",
        "\n",
        "        y_test_tmp = ys_test[y_col].dropna()\n",
        "        X_test_tmp = X_test.loc[y_test_tmp.index]\n",
        "\n",
        "        preds = models[f'model_horizon_{i+1}'].predict(X_test_tmp)\n",
        "\n",
        "        mape = mean_absolute_percentage_error(y_test_tmp, preds)\n",
        "        mae = mean_absolute_error(y_test_tmp, preds)\n",
        "        rmse = np.sqrt(mean_squared_error(y_test_tmp, preds))\n",
        "        count = len(preds)\n",
        "\n",
        "        df.loc[i] = {'MAPE': round(mape,4), 'MAE': round(mae,4), 'RMSE': round(rmse,4), 'Count': count}\n",
        "\n",
        "        print(f'Metrics for horizon {i+1}: MAPE: {mape:.4f}, MAE: {mae:.4f}, RMSE: {rmse:.4f}')\n",
        "\n",
        "    total_count = df['Count'].sum()\n",
        "\n",
        "    weighted_avg_mape = (df['MAPE'] * df['Count']).sum() / total_count\n",
        "    weighted_avg_mae = (df['MAE'] * df['Count']).sum() / total_count\n",
        "    weighted_avg_rmse = (df['RMSE'] * df['Count']).sum() / total_count\n",
        "\n",
        "    print(f\"\\nMetrics weighted mean: MAPE: {weighted_avg_mape:.4f}, MAE: {weighted_avg_mae:.4f}, RMSE: {weighted_avg_rmse:.4f}\")\n",
        "\n",
        "    #display(df)\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1i-tUUyfi2yR"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title N-models\n",
        "\n",
        "def add_shifted_targets(dataset, max_n):\n",
        "    \"\"\"\n",
        "    Tworzy zbiór danych z przesuniętymi targetami dla różnych horyzontów prognozy.\n",
        "    \"\"\"\n",
        "    target_columns = []\n",
        "    for n in range(1, max_n + 1):\n",
        "        dataset[f'price_t+n_{n}'] = dataset.groupby('name')['price'].shift(-n)\n",
        "        target_columns.append(f'price_t+n_{n}')\n",
        "\n",
        "    #dataset.drop(columns=['price'],inplace=True)\n",
        "    #dataset.rename(columns={'price': 'price_lag0'}, inplace=True)\n",
        "    return dataset, target_columns\n",
        "\n",
        "\n",
        "def train_models_for_forecast_horizon(X_n, y_n, target_cols, model_params,n_splits=4):\n",
        "\n",
        "    models = {}\n",
        "    for i, y_col in enumerate(target_cols):\n",
        "      target = y_n[y_col].copy().dropna()\n",
        "      X_n_tmp = X_n.loc[target.index]\n",
        "      #display(X_n_tmp)\n",
        "      #display(target)\n",
        "      #return\n",
        "      model, datasets = learnCV(X_n_tmp,target,n_splits)\n",
        "\n",
        "      models[f'model_horizon_{i+1}'] = model\n",
        "      if not TEST_MODE:\n",
        "        autosave_model(model)\n",
        "      del model\n",
        "      print(f'Model for horizon {i+1} trained.')\n",
        "\n",
        "    return models\n",
        "\n",
        "\n",
        "N = 7\n",
        "n_splits_n=2\n",
        "\n",
        "if not _features_added:\n",
        "  raise RuntimeError(\"Features not added\")\n",
        "\n",
        "dataset_n = dataset.copy()\n",
        "dataset_n, target_columns_n = add_shifted_targets(dataset_n, N)\n",
        "\n",
        "X_n = dataset_n.drop(columns=target_columns_n)\n",
        "y_n = dataset_n[target_columns_n]\n",
        "\n",
        "#X_train_n, X_test_n, y_train_n, y_test_n = split_dataset(X_n, y_n)\n",
        "\n",
        "models = train_models_for_forecast_horizon(X_n, y_n, target_columns_n, model_params_n,n_splits_n)\n",
        "\n",
        "\n",
        "test_index = get_test_index_TSS(X,n_splits_n)\n",
        "\n",
        "\n",
        "_=eval_models_n(models, X_n.iloc[test_index], y_n.iloc[test_index])\n",
        "\n",
        "del test_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "cellView": "form",
        "id": "O1UXt1TP6MLJ",
        "outputId": "552b2352-4e9e-43a0-eaf3-fcd152aea62b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:204: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037774 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 19447\n",
            "[LightGBM] [Info] Number of data points in the train set: 1893, number of used features: 74\n",
            "[LightGBM] [Info] Start training from score 1.380000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[50]\ttest's mape: 0.127217\ttest's l1: 0.437352\n",
            "[100]\ttest's mape: 0.106706\ttest's l1: 0.359862\n",
            "[150]\ttest's mape: 0.106486\ttest's l1: 0.359546\n",
            "Early stopping, best iteration is:\n",
            "[88]\ttest's mape: 0.107068\ttest's l1: 0.361003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:204: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008623 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25183\n",
            "[LightGBM] [Info] Number of data points in the train set: 3785, number of used features: 75\n",
            "[LightGBM] [Info] Start training from score 1.850000\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[50]\ttest's mape: 0.0965223\ttest's l1: 0.213964\n",
            "[100]\ttest's mape: 0.0962871\ttest's l1: 0.213104\n",
            "Early stopping, best iteration is:\n",
            "[48]\ttest's mape: 0.096627\ttest's l1: 0.214434\n",
            "Model for horizon 1 trained.\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003776 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 19255\n",
            "[LightGBM] [Info] Number of data points in the train set: 1860, number of used features: 74\n",
            "[LightGBM] [Info] Start training from score 1.340000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Training until validation scores don't improve for 100 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:204: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\ttest's mape: 0.186484\ttest's l1: 0.635358\n",
            "[100]\ttest's mape: 0.182604\ttest's l1: 0.62068\n",
            "[150]\ttest's mape: 0.178719\ttest's l1: 0.605013\n",
            "[200]\ttest's mape: 0.179372\ttest's l1: 0.606629\n",
            "Early stopping, best iteration is:\n",
            "[123]\ttest's mape: 0.178791\ttest's l1: 0.605078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:204: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007414 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25179\n",
            "[LightGBM] [Info] Number of data points in the train set: 3718, number of used features: 75\n",
            "[LightGBM] [Info] Start training from score 1.850000\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[50]\ttest's mape: 0.111608\ttest's l1: 0.254248\n",
            "[100]\ttest's mape: 0.111993\ttest's l1: 0.255223\n",
            "[150]\ttest's mape: 0.112132\ttest's l1: 0.254749\n",
            "Early stopping, best iteration is:\n",
            "[50]\ttest's mape: 0.111608\ttest's l1: 0.254248\n",
            "Model for horizon 2 trained.\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002599 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 19043\n",
            "[LightGBM] [Info] Number of data points in the train set: 1825, number of used features: 74\n",
            "[LightGBM] [Info] Start training from score 1.320000\n",
            "Training until validation scores don't improve for 100 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:204: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\ttest's mape: 0.186428\ttest's l1: 0.632023\n",
            "[100]\ttest's mape: 0.184143\ttest's l1: 0.621888\n",
            "[150]\ttest's mape: 0.184926\ttest's l1: 0.624038\n",
            "[200]\ttest's mape: 0.180968\ttest's l1: 0.610703\n",
            "[250]\ttest's mape: 0.179707\ttest's l1: 0.60648\n",
            "[300]\ttest's mape: 0.178958\ttest's l1: 0.603312\n",
            "[350]\ttest's mape: 0.172165\ttest's l1: 0.579469\n",
            "[400]\ttest's mape: 0.171114\ttest's l1: 0.575289\n",
            "[450]\ttest's mape: 0.169876\ttest's l1: 0.570551\n",
            "[500]\ttest's mape: 0.169549\ttest's l1: 0.569506\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[412]\ttest's mape: 0.169695\ttest's l1: 0.569949\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:204: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006588 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25170\n",
            "[LightGBM] [Info] Number of data points in the train set: 3650, number of used features: 75\n",
            "[LightGBM] [Info] Start training from score 1.840000\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[50]\ttest's mape: 0.115253\ttest's l1: 0.255298\n",
            "[100]\ttest's mape: 0.115114\ttest's l1: 0.25337\n",
            "Early stopping, best iteration is:\n",
            "[30]\ttest's mape: 0.115656\ttest's l1: 0.257027\n",
            "Model for horizon 3 trained.\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002335 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 18843\n",
            "[LightGBM] [Info] Number of data points in the train set: 1792, number of used features: 74\n",
            "[LightGBM] [Info] Start training from score 1.300000\n",
            "Training until validation scores don't improve for 100 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:204: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\ttest's mape: 0.224768\ttest's l1: 0.758759\n",
            "[100]\ttest's mape: 0.22428\ttest's l1: 0.756312\n",
            "[150]\ttest's mape: 0.219743\ttest's l1: 0.739839\n",
            "[200]\ttest's mape: 0.216198\ttest's l1: 0.726562\n",
            "[250]\ttest's mape: 0.215258\ttest's l1: 0.723606\n",
            "[300]\ttest's mape: 0.21376\ttest's l1: 0.718894\n",
            "[350]\ttest's mape: 0.214832\ttest's l1: 0.722144\n",
            "Early stopping, best iteration is:\n",
            "[272]\ttest's mape: 0.213527\ttest's l1: 0.718095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:204: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003903 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25169\n",
            "[LightGBM] [Info] Number of data points in the train set: 3583, number of used features: 75\n",
            "[LightGBM] [Info] Start training from score 1.840000\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[50]\ttest's mape: 0.121065\ttest's l1: 0.260699\n",
            "[100]\ttest's mape: 0.121161\ttest's l1: 0.260406\n",
            "Early stopping, best iteration is:\n",
            "[28]\ttest's mape: 0.121266\ttest's l1: 0.262981\n",
            "Model for horizon 4 trained.\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002653 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 18704\n",
            "[LightGBM] [Info] Number of data points in the train set: 1759, number of used features: 74\n",
            "[LightGBM] [Info] Start training from score 1.300000\n",
            "Training until validation scores don't improve for 100 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:204: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\ttest's mape: 0.292375\ttest's l1: 0.970997\n",
            "[100]\ttest's mape: 0.284042\ttest's l1: 0.945228\n",
            "[150]\ttest's mape: 0.273222\ttest's l1: 0.908862\n",
            "[200]\ttest's mape: 0.274849\ttest's l1: 0.914577\n",
            "[250]\ttest's mape: 0.268818\ttest's l1: 0.893342\n",
            "[300]\ttest's mape: 0.259673\ttest's l1: 0.86424\n",
            "[350]\ttest's mape: 0.259865\ttest's l1: 0.864734\n",
            "[400]\ttest's mape: 0.258746\ttest's l1: 0.86121\n",
            "[450]\ttest's mape: 0.255646\ttest's l1: 0.850422\n",
            "[500]\ttest's mape: 0.255382\ttest's l1: 0.849688\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[433]\ttest's mape: 0.255591\ttest's l1: 0.850258\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003819 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3516, number of used features: 75\n",
            "[LightGBM] [Info] Start training from score 1.850000\n",
            "Training until validation scores don't improve for 100 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:204: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\ttest's mape: 0.129593\ttest's l1: 0.282528\n",
            "[100]\ttest's mape: 0.131317\ttest's l1: 0.285675\n",
            "Early stopping, best iteration is:\n",
            "[25]\ttest's mape: 0.130231\ttest's l1: 0.285518\n",
            "Model for horizon 5 trained.\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003526 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 18534\n",
            "[LightGBM] [Info] Number of data points in the train set: 1728, number of used features: 74\n",
            "[LightGBM] [Info] Start training from score 1.290000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Training until validation scores don't improve for 100 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:204: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\ttest's mape: 0.319648\ttest's l1: 1.064\n",
            "[100]\ttest's mape: 0.317914\ttest's l1: 1.05849\n",
            "[150]\ttest's mape: 0.313303\ttest's l1: 1.04162\n",
            "[200]\ttest's mape: 0.313062\ttest's l1: 1.04067\n",
            "[250]\ttest's mape: 0.307642\ttest's l1: 1.02068\n",
            "[300]\ttest's mape: 0.304627\ttest's l1: 1.01078\n",
            "[350]\ttest's mape: 0.301627\ttest's l1: 1.00145\n",
            "[400]\ttest's mape: 0.300694\ttest's l1: 0.998834\n",
            "[450]\ttest's mape: 0.30117\ttest's l1: 1.00028\n",
            "Early stopping, best iteration is:\n",
            "[377]\ttest's mape: 0.300457\ttest's l1: 0.997449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:204: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003751 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25170\n",
            "[LightGBM] [Info] Number of data points in the train set: 3455, number of used features: 75\n",
            "[LightGBM] [Info] Start training from score 1.860000\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[50]\ttest's mape: 0.13776\ttest's l1: 0.29669\n",
            "[100]\ttest's mape: 0.137196\ttest's l1: 0.296617\n",
            "Early stopping, best iteration is:\n",
            "[48]\ttest's mape: 0.137784\ttest's l1: 0.296673\n",
            "Model for horizon 6 trained.\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002413 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 18452\n",
            "[LightGBM] [Info] Number of data points in the train set: 1697, number of used features: 74\n",
            "[LightGBM] [Info] Start training from score 1.290000\n",
            "Training until validation scores don't improve for 100 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:204: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\ttest's mape: 0.341365\ttest's l1: 1.13258\n",
            "[100]\ttest's mape: 0.330422\ttest's l1: 1.09809\n",
            "[150]\ttest's mape: 0.330777\ttest's l1: 1.09767\n",
            "[200]\ttest's mape: 0.327956\ttest's l1: 1.08824\n",
            "[250]\ttest's mape: 0.328472\ttest's l1: 1.08956\n",
            "Early stopping, best iteration is:\n",
            "[180]\ttest's mape: 0.326963\ttest's l1: 1.08556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:204: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004152 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25164\n",
            "[LightGBM] [Info] Number of data points in the train set: 3394, number of used features: 75\n",
            "[LightGBM] [Info] Start training from score 1.870000\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[50]\ttest's mape: 0.138594\ttest's l1: 0.297354\n",
            "[100]\ttest's mape: 0.1382\ttest's l1: 0.2978\n",
            "Early stopping, best iteration is:\n",
            "[43]\ttest's mape: 0.138549\ttest's l1: 0.297133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ygd7Ny4QHSi",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title N-iterative\n",
        "def predict_iterative(dataset,item_name, N):\n",
        "    if item_name not in dataset.index.get_level_values('name'):\n",
        "        print(f\"{item_name} is not in the given data\")\n",
        "        return None\n",
        "    item_data = dataset.xs(item_name, level='name', drop_level=False)\n",
        "\n",
        "    if len(item_data) == 0:\n",
        "        print(f\"{item_name} is not in the given data\")\n",
        "        return None\n",
        "\n",
        "    N = min(N, len(item_data) - 1)\n",
        "\n",
        "    item_actuals = item_data['price']\n",
        "    item_1ahead_pred = y_pred[item_data.index]\n",
        "\n",
        "    predictions = pd.DataFrame()\n",
        "    predictions_perfect = pd.DataFrame()\n",
        "\n",
        "    start_index = len(item_data) - N\n",
        "    end_index = min(start_index + N, len(item_data))\n",
        "\n",
        "    actual_prices = item_data.iloc[start_index:end_index]['price']\n",
        "    bef_curr_row = item_data.iloc[[start_index - 1]]\n",
        "\n",
        "    train_item_data = item_data.loc[:bef_curr_row.index[0]].copy()\n",
        "    perfect_item_data = item_data.loc[:bef_curr_row.index[0]].copy()\n",
        "    #display(train_item_data)\n",
        "    huj_counter=0\n",
        "    for i in range(start_index, end_index):\n",
        "        new_row = bef_curr_row.reset_index(drop=False)\n",
        "        new_row['date'] += pd.DateOffset(days=1)\n",
        "        new_row['price'] = np.nan\n",
        "        new_row.set_index(['name', 'date'], inplace=True)\n",
        "\n",
        "        train_item_data = pd.concat([train_item_data, new_row], ignore_index=False)\n",
        "        try:\n",
        "          perfect_item_data = pd.concat([perfect_item_data, item_data.loc[new_row.index]], ignore_index=False)\n",
        "        except KeyError as e:\n",
        "          huj_counter+=1\n",
        "          perfect_item_data=pd.concat([train_item_data, new_row], ignore_index=False)\n",
        "\n",
        "        train_item_data = add_features(train_item_data)\n",
        "        perfect_item_data = add_features(perfect_item_data)\n",
        "\n",
        "        X = train_item_data.iloc[[-1]].drop(columns=['price'])\n",
        "        X_perfect = perfect_item_data.iloc[[-1]].drop(columns=['price'])\n",
        "\n",
        "        prediction = round(model.predict(X, num_iteration=model.best_iteration)[0], 2)\n",
        "        prediction_perfect = round(model.predict(X_perfect, num_iteration=model.best_iteration)[0], 2)\n",
        "\n",
        "        train_item_data.at[new_row.index[0], 'price'] = prediction\n",
        "        perfect_item_data.at[new_row.index[0], 'price'] = prediction\n",
        "\n",
        "        bef_curr_row = train_item_data.iloc[[-1]]\n",
        "\n",
        "        predictions = pd.concat([predictions, pd.DataFrame({'price': [prediction]}, index=X.index)])\n",
        "        predictions_perfect = pd.concat([predictions_perfect, pd.DataFrame({'price': [prediction_perfect]}, index=X.index)])\n",
        "\n",
        "    print(\"huj_counter: \",huj_counter)\n",
        "    actual_prices_in_period = item_data.iloc[start_index:end_index]['price']\n",
        "    naive_prices_in_period = pd.Series(item_data['price_lag7'].fillna(X_test['storePrice']).fillna(3).iloc[start_index:end_index].to_numpy(), index=actual_prices_in_period.index)\n",
        "\n",
        "    results = {\n",
        "        \"Metric\": [\"Accuracy\", \"RMSE\", \"MAE\", \"MAPE\", \"Max Error\",\"Median Absolute Error\"],\n",
        "        \"N-iter\": calc_accuracy(predictions['price'], actual_prices_in_period),\n",
        "        \"N-iter_perfect\": calc_accuracy(predictions_perfect['price'], actual_prices_in_period),\n",
        "        \"1ahead\": calc_accuracy(item_1ahead_pred[start_index:end_index], actual_prices_in_period),\n",
        "        \"Naive\": calc_accuracy(naive_prices_in_period, actual_prices_in_period),\n",
        "        \"start_index\": [start_index] * 6,  # Adding start_index for each metric\n",
        "        \"end_index\": [end_index] * 6      # Adding end_index for each metric\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    return df, predictions, predictions_perfect, item_actuals, item_1ahead_pred, start_index, end_index\n",
        "\n",
        "\n",
        "def plot_results(item_actuals, item_1ahead_pred, predictions, predictions_perfect, start_index, end_index, item_name):\n",
        "  dates = item_actuals.index.get_level_values('date')\n",
        "  sample_len = end_index - start_index + 1\n",
        "  overhead = 15\n",
        "\n",
        "  if len(dates) > 60:\n",
        "      if sample_len + overhead <= 90:\n",
        "          display_start_idx = -(sample_len + overhead)\n",
        "      else:\n",
        "          display_start_idx = -(sample_len + max(overhead, int(0.5 * sample_len)))\n",
        "  else:\n",
        "      display_start_idx = None\n",
        "\n",
        "  plt.style.use('dark_background')\n",
        "  plt.figure(figsize=(12, 6))\n",
        "  plt.xticks(rotation=45)\n",
        "  plt.plot(dates[display_start_idx:], item_actuals.values[display_start_idx:], label='actual')\n",
        "  plt.plot(dates[display_start_idx:], item_1ahead_pred.values[display_start_idx:], label='prediction 1ahead')\n",
        "  plt.plot(dates[start_index:end_index], predictions_perfect.values, label='prediction iter, perfect data', color='green')\n",
        "  plt.plot(dates[start_index:end_index], predictions.values, label='prediction iter', color='red')\n",
        "  plt.xlabel('Data')\n",
        "  plt.ylabel('Cena')\n",
        "  plt.title(f'{item_name}: Forecasting Results')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "def get_summaries(dataset,test_names, N, plot=True):\n",
        "    summary_df = pd.DataFrame()\n",
        "    for item_name in test_names:\n",
        "        res=predict_iterative(dataset,item_name, N)\n",
        "        if res is None:\n",
        "            continue\n",
        "        result, predictions, predictions_perfect, item_actuals, item_1ahead_pred, start_index, end_index = res\n",
        "        if result is not None:\n",
        "            result['item_name'] = item_name\n",
        "            display(result)\n",
        "            summary_df = pd.concat([summary_df, result], ignore_index=True)\n",
        "            if plot:\n",
        "                plot_results(item_actuals, item_1ahead_pred, predictions, predictions_perfect, start_index, end_index, item_name)\n",
        "    return summary_df\n",
        "\n",
        "\n",
        "def summarize_summary(summary_df):\n",
        "    metrics = ['Accuracy', 'RMSE', 'MAE', 'MAPE', 'Max Error', 'Median Absolute Error']\n",
        "    methods = ['N-iter_perfect', '1ahead', 'Naive']\n",
        "    summary_stats = pd.DataFrame()\n",
        "\n",
        "    for method in methods:\n",
        "        grouped = summary_df[summary_df['Metric'].isin(metrics)].groupby('Metric',observed=True)\n",
        "\n",
        "        # Initialize DataFrame to hold weighted metrics\n",
        "        method_stats = pd.DataFrame(index=metrics)\n",
        "\n",
        "        # Calculate weighted mean and weighted std for each metric\n",
        "        for metric in metrics:\n",
        "            metric_data = summary_df[summary_df['Metric'] == metric][method]\n",
        "            weights = summary_df[summary_df['Metric'] == metric]['end_index'] - summary_df[summary_df['Metric'] == metric]['start_index']\n",
        "\n",
        "            weighted_mean = np.average(metric_data, weights=weights)\n",
        "\n",
        "            weighted_std = np.sqrt(np.average((metric_data - weighted_mean) ** 2, weights=weights))\n",
        "\n",
        "            method_stats.loc[metric, f'Weighted Mean_{method}'] = weighted_mean\n",
        "            method_stats.loc[metric, f'Weighted Std_{method}'] = weighted_std\n",
        "\n",
        "        method_stats[f'Mean_{method}'] = grouped[method].mean()\n",
        "        method_stats[f'Median_{method}'] = grouped[method].median()\n",
        "        summary_stats = pd.concat([summary_stats, method_stats], axis=1)\n",
        "\n",
        "    return summary_stats\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Launch N-iterative\n",
        "N_days=200\n",
        "\n",
        "random.seed(41)\n",
        "item_names = ['Black Diamond Thompson', 'Acid Rock',*[random.choice(test_names) for _ in range(8)]]  # Add your actual test names here\n",
        "\n",
        "summaries = get_summaries(test_dataset,item_names, N_days, plot=1)\n",
        "final_summary = summarize_summary(summaries)\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "display(final_summary)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#result, predictions, predictions_perfect, item_actuals, item_1ahead_pred,  start_index, end_index = foo('Black Diamond Thompson',30)\n",
        "#result['item_name'] = item_name\n",
        "#display(result)\n",
        "#plot_results(item_actuals, item_1ahead_pred, predictions, predictions_perfect, start_index, end_index, item_name)"
      ],
      "metadata": {
        "id": "amHJ_FnLwlqi",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "collapsed": true,
        "id": "ZN-YlhrZ6n4A",
        "outputId": "4c001f5d-39ec-4790-9f53-6bfe9f0c7229"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'test_names' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-7c67387e6fd7>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtshow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actual'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mphs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mend_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actual'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'pred'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_names' is not defined"
          ]
        }
      ],
      "source": [
        "#@title plot predictions\n",
        "\n",
        "tshow=X_test.copy()\n",
        "tshow['pred1'] = y_pred\n",
        "\n",
        "tshow['actual'] = y_test\n",
        "for name in test_names[::9][:7]:\n",
        "  phs=[]\n",
        "  end_names=['actual','pred']\n",
        "\n",
        "  try:\n",
        "    ph = tshow.loc[name][['actual','pred1']]\n",
        "    phs=[ph[['pred1']],ph[['actual']]]\n",
        "  except KeyError as e:\n",
        "    print(name,\" not in dataset\")\n",
        "    raise e\n",
        "    continue\n",
        "\n",
        "  end_names.append(name)\n",
        "\n",
        "  plot_pricehistory(phs,end_names,name+' test')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dXbn3Hqe8L5_"
      },
      "outputs": [],
      "source": [
        "trim=0\n",
        "print_importances(model,trim,False,figsize=(10, 18))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(models)):\n",
        "  trim=10\n",
        "  print_importances(models[f\"model_horizon_{i+1}\"],0,False,name=f\" N={i+1}'\",figsize=(7, 2),end=10)\n"
      ],
      "metadata": {
        "id": "NiGGXndilQZM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93R2Z9skV55g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaZch1suzFkB"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObQVhB0gY4jVg3IvtEEBms",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}